# PySpark 窗口函数

> 原文：<https://www.educba.com/pyspark-window-functions/>

![PySpark Window Functions](img/b3d56308b3f4a9e0c11d5e6bc48ec152.png)



## PySpark 窗口简介

PySpark window 是一个 Spark 函数，用来计算带有数据的窗口函数。正常的窗口函数包括用于对输入行进行操作并生成结果的函数，如秩、行号。它也被称为窗口或窗口函数，通常对一组行执行计算，这一行可以称为帧。聚合或应用函数后，将为每一行返回一个新值，该值将与给定的值相对应。我们还可以在 PySpark 数据框架上使用 SQL 相关的查询，并应用同样的方法。窗口操作作用于一组行，并通过应用聚合函数为每个输入行返回一个值。有几种窗口操作可以应用于数据来计算结果。

### 带示例的窗口函数

下面给出的是带示例的窗口函数:

<small>网页开发、编程语言、软件测试&其他</small>

#### 1.排名函数

这些是 PySpark 中的窗口函数，用于对数据进行排序。有几个用于处理数据和计算结果的排名函数。

让我们详细检查一些排名功能。

**a. ROW_NUMBER():** 这给出了该行的行号。它从 1 开始，行数根据分区元素不断增加。

让我们从制作一个简单的数据框开始，我们将尝试在这个数据框上实现窗口功能。考虑了一个有关系和所有学期成绩的学生的数据框架，并在此基础上制作了数据框架。

**代码:**

`data1 = (("Bob", "IT", 4500), \
("Maria", "IT", 4600), \
("James", "IT", 3850), \
("Maria", "HR", 4500), \
("James", "IT", 4500), \
("Sam", "HR", 3300), \
("Jen", "HR", 3900), \
("Jeff", "Marketing", 4500), \
("Anand", "Marketing", 2000),\
("Shaid", "IT", 3850) \
)
col= ["Name", "MBA_Stream", "SEM_MARKS"] >>> b = spark.createDataFrame(data1,col)`

create data 根据列和数据创建数据框。

`>>> b.printSchema()
>>> from pyspark.sql.window import Window`

用于窗口操作的窗口函数。

`>> from pyspark.sql.functions import row_number`

Row_number 窗口函数计算基于分区的行数。

`>> w = Window.partitionBy("MBA_Stream").orderBy("Name")`

要使用的列和要用于的 order by 操作。

`>> b.withColumn("Windowfunc_row",row_number().over(w)).show()`

这将创建一个数据框，并使用 row_number 窗口函数来计算给定数据框的 row_number。

**输出:**

![PySpark Window Functions 1](img/8f4b9ab3bd7584c48922e32e02695aca.png)



![PySpark Window Functions 3](img/f2d9146d7574561aca06b983376fc811.png)



**b .秩函数:**该函数用于提供给定数据帧的秩。这是一种窗口操作，用于根据数据框创建等级。

我们将尝试从 SQL Rank 函数中导入 Rank 函数。

**代码:**

`>> from pyspark.sql.functions import rank`

这将导入排名函数，我们可以在其中计算排名。

`>> b.withColumn("Window_Rank",rank().over(w)).show()`

这将按照建议对给定条件下的元素进行排序。

**输出:**

![RANK](img/dd8b0551f57b78a9315c7f6d87f414f6.png)



**c. DENSE-RANK 函数:**类似于 RANK 函数，这也用于对元素进行排序，但是不同之处在于排序没有任何间隙。

我们将引入稠密秩函数。

**代码:**

`>>> from pyspark.sql.functions import dense_rank
>>> b.withColumn("Window_DeneRank",dense_rank().over(w)).show()`

**输出:**

![DENSE-RANK](img/943c0d757d0d27b9bc17e7b461f6310f.png)



**d. NTILE 函数:**它返回结果的相对排名，它有一个参数值，排名元素将位于其中。

**举例:**

**代码:**

`>>> from pyspark.sql.functions import ntile
>>> b.withColumn("Window_Ntile",ntile(2).over(w)).show()`

这里我们给出的参数是 2，它只根据这个值对函数进行排序。

**输出:**

![NTILE](img/ed3552637c081e549eef60b9445868f4.png)



#### 2.分析功能

这些是用于数据流分析的窗口函数。

让我们来看看一些分析功能:

**a. Cume_dist()函数:**给出分区上值的累积分布。

**代码:**

`>>> b.withColumn("Window_cumeDist",cume_dist().over(w)).show()`

**输出:**

![PySpark Window Functions 6JPG](img/0474574c2f1463abcbbef7940789090e.png)



**b. LAG 函数:**这是一个窗口函数，用于从定义的偏移值开始访问之前的数据。

**举例:**

**代码:**

`>>> b.withColumn("Window_lag",lag("SEM_MARKS",1).over(w)).show()`

这给出了所用列的先前值。

同样的偏移量可以调整，我们可以取需要的值。

`>>> b.withColumn("Window_lag",lag("SEM_MARKS",2).over(w)).show()`

**输出:**

![LAG](img/2bce5f4a1af7b4c5a782c03b46de2eea.png)



**c. LEAD Function:** 这是一个窗口函数，用于从定义的偏移值开始访问下一个数据。

**代码:**

`>>> from pyspark.sql.functions import lead
>>> b.withColumn("Window_lead",lead("SEM_MARKS",1).over(w)).show()`

**输出:**

![LEAD](img/cb3f8945cf7783cc3bc44d9a823fca7c.png)



从这里我们可以看到 PySpark 中不同窗口函数的使用。

### 结论

从上面的文章中我们看到了 PySpark 中窗口函数的使用。从各种例子和分类中，我们试图理解窗口方法在 PySpark 中是如何工作的，以及在编程级别中使用了什么。工作模式使我们正确理解了职能的内涵，并帮助我们获得了更多的相关知识。语法有助于检查所使用的参数和函数的功能知识。我们还看到了 PySpark 的内部工作原理和在 Spark 数据框架中使用 Window 函数的优点，以及它在各种编程目的中的用法。语法和例子也帮助我们更精确地理解这个函数。

### 推荐文章

这是 PySpark 窗口函数的指南。这里我们分别用例子讨论介绍和窗口功能。您也可以看看以下文章，了解更多信息–

1.  [星火中的 RDD](https://www.educba.com/rdd-in-spark/)
2.  [火花广播](https://www.educba.com/spark-broadcast/)
3.  [火花上下文](https://www.educba.com/sparkcontext/)
4.  [PySpark 加入](https://www.educba.com/pyspark-join/)





