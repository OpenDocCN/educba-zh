# 火花执行器

> 原文:[https://www.educba.com/spark-executor/](https://www.educba.com/spark-executor/)

![Spark Executor](../Images/730c69feceadd9f7d466efe2612b119c.png)

<noscript><img class="alignnone size-full wp-image-374119" src="../Images/730c69feceadd9f7d466efe2612b119c.png" alt="Spark Executor" width="900" height="498" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Executor.png 900w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Executor-300x166.png 300w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Executor-768x425.png 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Executor.png"/></noscript>

## 火花执行器简介

有一个称为 spark executor 的分发代理，负责执行给定的任务。spark 中的执行器是工人节点，通过负责给定的 Spark 作业来帮助运行单个任务。这些在 Spark 应用程序开始时启动，任务一运行，结果就立即发送给驱动程序。内存中由 Spark RDD 的执行程序提供的存储空间，由程序通过块管理器缓存。在一个完整的生命周期内运行应用程序，由此推断出 executor 的静态分配。

### Apache Spark Executor 是如何工作的？

每次得到一个事件，执行器就重新启动一个应用程序。使用该执行器时会使用纱线上的火花，这通常与 Mesos 不兼容。使用 Spark executor 可以以任何方式完成，如开始运行 Sparkafter MapR FS、Hadoop FS 或 Amazon S# destination close 文件的应用程序。每当 Hadoop FS 目标关闭一个文件时，spark 应用程序每次都会将 Arvo 文件转换为 Parquet。在外部系统中，启动 Spark 应用程序。它既不等待监控应用程序也不完成。成功提交申请后，会进行额外的处理。

<small>Hadoop、数据科学、统计学&其他</small>

![Apache Spark Executor Works](../Images/352a313492775074cc23b041b9824c68.png)

<noscript><img class="alignnone size-full wp-image-374121" src="../Images/352a313492775074cc23b041b9824c68.png" alt="Apache Spark Executor Works" width="602" height="323" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Apache-Spark-Executor-Works.png 602w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Apache-Spark-Executor-Works-300x161.png 300w" sizes="(max-width: 602px) 100vw, 602px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Apache-Spark-Executor-Works.png"/></noscript>

it 可以使用客户端或集群模式来运行应用程序。但是只有当资源的使用不是一个需要考虑的交易时，才使用客户端模式。确保在使用 Spark executor 之前执行任务先决条件。在配置执行器之前，必须指定工作节点的数量。通过指定范围内所需的最大和最小节点数，也可以选择启用动态内存分配。此外，通过指定执行器和应用程序驱动程序所需的最小内存量，可以向它传递额外的集群管理器属性。

指定定制的 Java 主目录和 Spark 目录、Hadoop 的代理用户以及 Kerberos 凭证。可以指定用于编写应用程序的语言，然后根据定义的特定语言指定属性。执行器可以为变量事件流生成事件，变量事件流也可以配置。

### 创建 Spark 执行器的条件

当收到消息–registered executor 时，由粗粒度 ExecutorBackend。这种情况只发生在纱线中。

在 Spark 中的 Mesos 中注册 MesosExecutorBackend 时。

对于本地模式，将创建一个本地端点。

**代码:**

`at org.apache.spark. SparkContext$Sanonfun$assertNoOtherContextIsRunning$2.appl Que
at org.apache.spark.SparkContext anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:245
at scala.Option.foreach
at org.apache.spark.SparkContext.assertNoOtherContext IsRunning
at org.apache.spark.SparkContext.mark Partially Constructed
at org.apache.spark. SparkContext.<init>
scala
sc.stop
val conf = new SparkConf.
setAppName("testing").
setMaster("yarn").
set("spark.executor.instances", "2") conf: org.apache.spark.SparkConf - org.apache.spark. SparkConfe69d7b1f9
val sc = new SparkContext(conf) sc: org.apache.spark. SparkContext - org.apache.spark. SparkContext e31b4e1
sc.stop
scala import org.apache.spark.sql.SparkSession SparkSession SparkSession Extensions
import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SparkSession
scala val spark Spark SparkConf SparkContext SparkSession
scala val spark SparkSession.
Builder clearActiveSession getactiveSession builder clear Default Session getDefault Session
spark = SparkSession.builder.
appName config enableHiveSupport getorCreate
val spark
SparkSession.builder. // hit tab
val spark = SparkSession.builder.config("spark.executor.instances", "2".
master("yarn").
appName("testing).
val spark = SparkSession.builder.config("spark.executor.instances", "2").
master("yarn").
appName("testing").
getOrCreate`

**输出:**

![local endpoint](../Images/5a81796a35a6c6118385ee9f37fdef15.png)

<noscript><img class="alignnone wp-image-373467 size-full" src="../Images/5a81796a35a6c6118385ee9f37fdef15.png" alt="local endpoint" width="603" height="192" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-executor1.jpg 603w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-executor1-300x96.jpg 300w" sizes="(max-width: 603px) 100vw, 603px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-executor1.jpg"/></noscript>

![Spark executor2](../Images/8fd58c4f95eed348e53ea7a2cc4026de.png)

<noscript><img class="alignnone wp-image-373468 size-full" src="../Images/8fd58c4f95eed348e53ea7a2cc4026de.png" alt="Spark executor2" width="605" height="268" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-executor2.jpg 605w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-executor2-300x133.jpg 300w" sizes="(max-width: 605px) 100vw, 605px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-executor2.jpg"/></noscript>

### Spark 中 Apache Executor 的优点和使用注意事项

1.当应用程序在多线程任务中运行时，每个应用程序都有自己的执行过程。
2。对于底层集群管理器，spark 执行器是不可知的。意思就是只要流程做好了，彼此的沟通也就做好了。3。接受来自所有其他执行器的传入连接。
4。执行器应该在靠近工作节点的地方运行，因为驱动程序在集群上调度任务。这些可以优选地在相同的局域网中运行。

### 结论

我们已经看到了 Apache Spark 的 Spark Executor 的概念。我还看到了需要记住的关键点，以及执行者在执行任务时是如何发挥作用的。并且作为结论，可以说 Apache Spark 中的 Spark 执行器可以增强系统的性能。

### 推荐文章

这是一个星火执行者的指南。这里我们讨论一个关于 Spark Executor 的介绍，它是如何工作的，条件，优点，以及用途。您也可以浏览我们的其他相关文章，了解更多信息——

1.  [火花元件](https://www.educba.com/spark-components/)
2.  [火花功能](https://www.educba.com/spark-functions/)
3.  [火花工具](https://www.educba.com/spark-tools/)
4.  [火花版本](https://www.educba.com/spark-versions/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>