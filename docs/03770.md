# 简单线性回归

> 原文:[https://www.educba.com/simple-linear-regression/](https://www.educba.com/simple-linear-regression/)

![simple linear regressions](../Images/1fe0e6a55bb1cabc2e9b586e2425f2bd.png)

<noscript><img class="alignnone size-full wp-image-237402" src="../Images/1fe0e6a55bb1cabc2e9b586e2425f2bd.png" alt="simple linear regressions" width="834" height="469" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/11/simple-linear-regressions.png 834w, https://cdn.educba.com/academy/wp-content/uploads/2019/11/simple-linear-regressions-300x169.png 300w, https://cdn.educba.com/academy/wp-content/uploads/2019/11/simple-linear-regressions-768x432.png 768w" sizes="(max-width: 834px) 100vw, 834px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/11/simple-linear-regressions.png"/></noscript>

## 简单线性回归简介

以下文章提供了简单线性回归的概述。

回到以前或欠发达的状态。

<small>Hadoop、数据科学、统计学&其他</small>

**在统计学中:**一个变量的平均值与其他变量的相应值之间关系的量度。

在回归中，输入变量(自变量)和目标变量(因变量)之间的关系被认为是线性的，这种回归称为线性回归。简单线性回归是一种线性回归，其中我们只有一个自变量来预测因变量。简单线性回归是机器学习算法之一。简单线性回归属于监督学习家族。回归用于预测连续值。

### 简单线性回归模型

简单点说吧。这一切是如何开始的？

这一切都始于 1800 年的弗朗西斯·高尔顿。他研究了父亲和儿子的身高关系。他观察到一种模式:要么儿子的身高会和父亲一样高，要么儿子的身高会更接近所有人的整体平均身高。这种现象无非是退步。

**举例:**

沙克·奥尼尔是非常著名的 NBA 球员，身高 2.16 米。他的儿子沙奇尔(Shaqir)和沙里夫·奥尼尔(Shareef O'neal)身高分别为 1.96 米和 2.06 米。人口平均身高 1.76 米。儿子的身高回归平均身高。

### 我们如何回归？

计算只有两个数据点的回归:

![Simple Linear Regression1](../Images/e44835699ec3fef77cb2ad023f4af435.png)

<noscript><img class="alignnone wp-image-234348 size-full" src="../Images/e44835699ec3fef77cb2ad023f4af435.png" alt="Simple Linear Regression1" width="206" height="157" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear1.png"/></noscript>

我们希望找到最佳回归来画一条尽可能接近每个点的线。在两个数据点的情况下，很容易画一条线；加入他们吧。

现在，如果我们现在有许多数据点，如何画出尽可能接近每个数据点的线。

![Simple Linear Regression2](../Images/c874a42b8309e30816305b1f66fb035e.png)

<noscript><img class="alignnone wp-image-234362 size-full" src="../Images/c874a42b8309e30816305b1f66fb035e.png" alt="Simple Linear Regression2" width="289" height="186" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear2-1.png"/></noscript>

在这种情况下，我们的目标是最小化直线和所有数据点之间的垂直距离。这样，我们可以预测线性回归模型的最佳直线。

### 简单线性回归能做什么？

下面是简单线性回归的详细解释:

*   它画出很多很多可能的线条，然后进行任何分析。
*   误差平方和。
*   绝对误差之和。
*   最小二乘法…等等。
*   对于我们的分析，我们将使用最小二乘法。
*   我们将对所有的点进行差分，并计算所有的点的和。无论哪条线给出的总和最小，都将是我们的最佳线。

例:通过这样做，我们可以得到多个男人和他们儿子的身高，并且可以告诉一个男人他的儿子可能有多高。在他出生之前。

![Simple 3](../Images/8985800c0bdcab65c170a8c4080002bd.png)

<noscript><img class="alignnone wp-image-234382 size-full" src="../Images/8985800c0bdcab65c170a8c4080002bd.png" alt="Simple 3" width="275" height="211" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear3.png"/></noscript>

Google Image

上图显示了一个简单的线性回归。这条线代表回归线。由下式给出:y = a + b * x。

**其中 y 为因变量(DV):** 例如，一个人的工资如何根据员工的工作年限而变化。所以在这里，雇员或个人的工资将是你的因变量。

因变量是我们的目标变量，我们希望使用线性回归来预测的变量。

**x 是我们的自变量(IV):** 因变量是改变自变量的原因。在上面的例子中，工作经验的年数是我们的因变量，因为工作经验的年数导致了员工工资的变化。

![simple 4](../Images/6b93d5ad774d7a051e9df0f66f53b2ab.png)

<noscript><img class="alignnone wp-image-234396 size-full" src="../Images/6b93d5ad774d7a051e9df0f66f53b2ab.png" alt="simple 4" width="266" height="193" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear4.png"/></noscript>

*   b 是自变量 x 的系数变量，这个系数起着至关重要的作用。它说明了 x (IV)的单位变化如何影响 y (DV)。它也被称为比例系数。从数学的角度来说，这取决于直线的斜率，或者你可以说直线很陡。
*   在我们的例子中，如果斜率(b)更小，这意味着年数将产生更少的工资增量；另一方面，如果斜率(b)更大，随着经验年数的增加，工资会有更高的增长。
*   a 是一个常数值。它也被称为截距，即直线与 y 轴或 DV 轴相交的位置。换句话说，我们可以说当一个雇员的工作经验(x)为零时，那么这个雇员的工资(y)将是常数(a)。

### 最小二乘法是如何工作的？

以下是最小二乘法的要点:

*   它根据数据趋势画了一条任意的线。
*   它获取数据点并绘制垂直线。它将垂直距离视为一个参数。
*   这些垂直线将切割回归线，并给出数据点的相应点。
*   然后，它将找到每个数据点与其在回归线上对应的数据点之间的垂直差异。
*   它将计算误差，即差值的平方。
*   然后，它计算误差的总和。
*   然后，它会画一条线，并再次重复上述程序。
*   它以这种方式画出许多线条，给出最小误差和的线条被选为最佳线条。
*   这条最好的线是我们简单的线性回归线。

### 简单线性回归的应用

进行回归分析以预测连续变量。回归分析有广泛的应用。

一些例子如下:

*   预测分析。
*   营销的有效性。
*   任何列表的定价。
*   产品促销预测。

这里我们将讨论线性回归在预测分析中的一个应用。我们将使用 python 建模。

#### 我们将按照以下步骤构建我们的模型:

*   我们将导入库和数据集。
*   我们将对数据进行预处理。
*   我们将数据分为测试集和训练集。
*   我们将创建一个模型，尝试根据我们的训练集预测目标变量。
*   我们将预测测试集的目标变量。
*   我们将分析模型预测的结果

在我们的分析中，我们将使用一个包含 30 名员工数据的工资数据集。

**#导入库**

**代码:**

`import numpy as np
import matplotlib.pyplot as plt
import pandas as pd`

**#导入数据集(数据样本见表)**

**代码:**

`dataset = pd.read_csv('Salary_Data.csv')`

| **年经验** | **工资** |
| One point five | Thirty-seven thousand seven hundred and thirty-one |
| One point one | Thirty-nine thousand three hundred and forty-three |
| Two point two | Thirty-nine thousand eight hundred and ninety-one |
| Two | Forty-three thousand five hundred and twenty-five |
| One point three | Forty-six thousand two hundred and five |
| Three point two | Fifty-four thousand four hundred and forty-five |
| Four | Fifty-five thousand seven hundred and forty-nine |

**#数据集预处理，这里我们将数据集分为因变量和自变量。x 为自变量，y 为因变量或目标变量。**

**代码:**

`X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values`

**#将数据集分成训练集和测试集:**

**代码:**

`from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)`

这里，测试大小 1/3 表明，从总数据来看，2/3 部分用于训练模型，其余 1/3 用于测试模型。

**#让我们用简单的线性回归模型来拟合训练集**

**代码:**

`from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)`

线性回归模型现在已经训练好了。这个模型将用于预测因变量。

**#预测测试集结果**

**代码:**

`y_pred = regressor.predict(X_test)`

**#可视化测试集结果**

**代码:**

`plt.scatter(X_test, y_test, color = 'blue')
plt.plot(X_train, regressor.predict(X_train), color = 'red')
plt.title('Salary of Employee vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()`

![simple 5](../Images/2c0fb35678bd4bebf703c5c1407de6d1.png)

<noscript><img class="alignnone wp-image-234419 size-full" src="../Images/2c0fb35678bd4bebf703c5c1407de6d1.png" alt="simple 5" width="338" height="266" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear5.png 338w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear5-300x236.png 300w" sizes="(max-width: 338px) 100vw, 338px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/simple-linear5.png"/></noscript>

**#模型参数**

**代码:**

`print(regressor.intercept_)
print(regressor.coef_)
26816.19224403119
[9345.94244312]`

所以拦截器(a)的值是 26816。这表明任何一个新人(零经验)的薪水都在 26816 英镑左右。

我们模型的系数是 9345.94。它表明，保持所有其他参数不变，一个单位的自变量(多年的经验)的变化。)将产生 9345 个单位的工资变化。

### 回归评估指标

基本上有 3 种重要的评估度量方法可用于回归分析:

*   **平均绝对误差(MAE):** 显示绝对误差的平均值，即预测值与实际值之差。
*   **均方误差(MSE):** 显示均方误差的平均值。
*   **均方根误差(RMSE):** 显示平方误差平均值的平方根。

#### 我们可以比较一下上面的方法:

*   **MAE:** 它显示了三种方法的平均误差和最简单的方法。
*   MSE: 这个比 MAE 更受欢迎，因为它增强了较大的误差，结果显示了更多的洞察力。
*   RMSE: 这个比 MSE 好，因为我们可以用 y 来解释误差

这三个都是损失函数。

**#模型评估**

**代码:**

`from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
MAE: 3426.4269374307123
MSE: 21026037.329511296
RMSE: 4585.4157204675885`

### 结论

线性回归分析是机器学习算法的强大工具，用于预测工资、销售、绩效等连续变量。线性回归考虑自变量和因变量之间的线性关系。简单线性回归只有一个独立变量，模型基于该变量预测目标变量。我们讨论了线性回归的模型和应用，并给出了一个预测员工工资的预测分析实例。

### 推荐文章

这是简单线性回归的指南。这里我们讨论线性回归的模型和应用，使用一个预测分析例子来预测雇员的工资。您也可以浏览我们的其他相关文章，了解更多信息——

1.  [线性回归数据集](https://www.educba.com/dataset-for-linear-regression/)
2.  [Matlab 线性回归](https://www.educba.com/matlab-linear-regression/)
3.  [NumPy 线性回归](https://www.educba.com/numpy-linear-regression/)
4.  [线性回归分析](https://www.educba.com/linear-regression-analysis/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>