# PySpark 读取 CSV

> 原文：<https://www.educba.com/pyspark-read-csv/>

![PySpark Read CSV](img/450974004ad64ff349ed870f47fd923d.png)



## PySpark Read CSV 简介

Pyspark read CSV 为数据帧的阅读器提供了一个 CSV 的路径来读取 Pyspark 的数据帧中的 CSV 文件，以便保存或写入 CSV 文件。使用 PySpark read CSV，我们可以从目录中读取单个或多个 CSV 文件。PySpark 将支持通过使用空格、制表符、逗号和任何我们在 CSV 文件中使用的分隔符来读取 CSV 文件。

### 概观

apache spark 中的数据帧将被定义为一个分布式集合，我们可以认为数据是通过使用命名列来组织的。数据框相当于关系数据库中的表或 python 语言的数据框。数据框是使用各种来源在结构化数据文件中构建的。

<small>网页开发、编程语言、软件测试&其他</small>

Apache PySpark 提供了用于读取 Spark 数据帧中的 CSV 文件的 CSV 路径，以及用于写入和保存指定 CSV 文件的 spark 数据帧的对象。在读取和写入 CSV 文件中的数据帧时，pyspark CSV 中有多个选项可用。在使用 pyspark read CSV 时，我们使用分隔符选项。分隔符用于指定 CSV 文件的列的分隔符；默认情况下，pyspark 会将其指定为逗号，但我们也可以将其设置为任何其他分隔符类型。

### 如何使用 PySpark 读取 CSV 数据？

我们可以在 PySpark 中使用单个或多个 CSV 文件进行读取。我们需要遵循以下步骤来使用文件数据。首先，我们需要在我们的系统中安装 PySpark。

*   在下面的例子中，我们使用 pip 命令在系统中安装 PySpark，如下所示。

**代码:**

```
pip install pyspark 
```

![PySpark Read CSV 1](img/765b72428e3015c72782e2523cec47bb.png)



*   在安装完 pyspark 模块后，我们登录 python shell，如下所示。

**代码:**

```
python
```

![PySpark Read CSV 2](img/0adba75ec9e1d82ba3332224eaecf1b7.png)



*   在 python shell 中登录后，我们导入所需的包，这是我们读取 CSV 文件所需要的。我们正在导入 spark 会话、管道、行和令牌化器包，如下所示。

**代码:**

```
from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer
from pyspark.sql import Row
```

![PySpark Read CSV 3](img/d276fef6ba8bb5c40b9e46885875f821.png)



*   在这一步中导入模块后，我们将变量定义为以 PY 格式读取 CSV 文件。

**代码:**

```
py = SparkSession.builder.appName ('Pyspark read csv').getOrCreate ()
```

![PySpark Read CSV 4](img/e4775d6efef6f789f28dabef737686fd.png)



*   在这一步中定义变量后，我们将 CSV 名称作为 pyspark 加载，如下所示。

**代码:**

```
read_csv = py.read.csv('pyspark.csv')
```

![PySpark Read CSV 5](img/8850bc09d3d3dd17c5a86a9451d464e6.png)



*   在此步骤中，CSV 文件从 CSV 文件中读取数据，如下所示。

**代码:**

```
rcsv = read_csv.toPandas()
rcsv.head()
```

![PySpark Read CSV 6](img/c3587edadce00962ea03714902222175.png)



### Pyspark 读取多个 CSV 文件

通过使用 read CSV，我们可以在一个代码中读取单个或多个 CSV 文件。要读取多个 CSV 文件，我们需要在定义 CSV 文件的路径时给出多个文件名。在下面的例子中，我们使用了如下两个文件。

**代码:**

```
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
spark_csv = SparkSession.builder.appName('Pyspark read multiple csv').getOrCreate()
path_csv = ['authors.csv',  'authors.csv']
file_csv = spark_csv.read.csv()
df_csv = file_csv.toPandas()
df_csv.head()
df_csv.tail()
```

![Pyspark read Multiple CSV Files](img/d28a12b84f9299a7d97a80fda051ca10.png)



在下面的例子中，我们用一段代码读取三个文件，如下所示。我们使用如下三个不同的文件。

**代码:**

```
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
spark_csv = SparkSession.builder.appName('Pyspark read multiple csv').getOrCreate()
path_csv = [spark1.csv',  spark2.csv', 'spark3.csv']
file_csv = spark_csv.read.csv()
df_csv = file_csv.toPandas()
df_csv.head()
df_csv.tail()
```

![reading three files](img/c0ca552482776545a3d9d8b393971840.png)



### Pyspark 阅读多种习俗

在创建数据框时，通过使用 pyspark，我们可以使用 struct 类型和类名作为 struct 字段来指定自定义结构。结构类型是用于定义列名的结构字段的集合。下面的例子显示 pyspark 读取多个海关如下。

**代码:**

```
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
custom_spark = SparkSession.builder.master ("local[1]") \
.appName ('SparkByExamples.com') \ .getOrCreate()
student = [("AB", "", "CD", 25),  ("PQ", "QR", "", 15),  ("XY", "", "YZ", 35) ]
stud_schema = StructType ([ \
StructField ("stud_fname", StringType (), True), \
StructField ("stud_mname", StringType (), True), \
StructField ("stud_lname", StringType (), True), \
StructField ("stud_id", IntegerType (), True) \ ])
stud = custom_spark.createDataFrame (data = student, schema = stud_schema)
stud.printSchema()
```

![Multiple customs](img/8b9060616dd0109d9592d42f7768b6e8.png)



### Pyspark 读取目录

我们也可以从指定的目录中读取所有的 CSV 文件。为了从指定目录中读取所有 CSV 文件，我们使用“*”符号。在下面的例子中，我们在目录中保存了两个文件，如下所示。

**代码:**

```
from pyspark.sql import SparkSession
spark_csv = SparkSession.builder.appName ('Read all').getOrCreate()
all_csv = spark_csv.read.csv()
read_all = all_csv.toPandas()
read_all.head()
read_all.tail()
```

![Read Directory](img/1a4a9eadc802b5828c9aa4ab0fa4fbee.png)



以下示例显示了 PySpark 读取目录。我们将三个文件放在一个指定的目录中，如下所示。

**代码:**

```
from pyspark.sql import SparkSession
spark_csv = SparkSession.builder.appName ('Read all').getOrCreate()
all_csv = spark_csv.read.csv()
read_all = all_csv.toPandas()
read_all.head()
read_all.tail()
read_all.head()
```

![Specified directory](img/ecca3aaacf66d454b5a77539c135d795.png)



### 例子

以下是不同的例子:

#### 示例#1

在本例中，我们使用一个 CSV 文件。

**代码:**

```
from pyspark.sql import SparkSession
single = SparkSession.builder.appName('Pyspark read csv').getOrCreate()
single_csv = single.read.csv()
rcsv = single_csv.toPandas()
rcsv.head()
```

![Example 1](img/2a7f65813dd2cbf1d616a33f4369eed3.png)



#### 实施例 2

以下示例显示 PySpark 火花读取 CSV 如下。我们使用两个 CSV 文件。

**代码:**

```
from pyspark.sql import SparkSession
two_csv = SparkSession.builder.appName ()
path_two = ['spark1.csv', 'spark2.csv']
file_two = two_csv.read.csv (path_two, sep=',')
df_two = file_two.toPandas()
df_two.head()
df_two.tail()
```

![Example 2](img/7b076142a9c95d33d0dade74e5343f66.png)



### 关键要点

*   我们在使用 PySpark 读取 CSV 文件时使用了多个选项。推断架构选项告诉读者从源文件推断数据类型。
*   我们可以在单个或多个文件上使用它，也可以读取所有的 CSV 文件。

### 常见问题解答

下面是提到的常见问题:

#### Q1。为什么我们使用 PySpark read CSV？

**答:**基本上它的用途就是读取指定的 CSV 文件。通过使用 spark，我们可以读取单个或多个 CSV 文件，也可以读取所有 CSV 文件。

#### Q2。PySpark read CSV 中的分隔符有什么用？

**答:**该选项用于指定 CSV 文件中某一列的分隔符，默认为逗号。

#### Q3。PySpark 中的 header 参数有什么用？

**答:**header 参数用于读取我们在代码中定义的文件的第一行。

### 结论

在读取和写入 CSV 文件中的数据帧时，PySpark CSV 中有多个选项可用。Pyspark 读取 CSV，提供一个 CSV 的路径给数据帧的阅读器来读取 PySpark 的数据帧中的 CSV 文件，用于保存或写入 CSV 文件。

### 推荐文章

这是 PySpark 读取 CSV 的指南。在这里，我们讨论了简介以及如何使用 PySpark 读取 CSV 数据，并给出了不同的示例。您也可以看看以下文章，了解更多信息–

1.  [PySpark 写 CSV](https://www.educba.com/pyspark-write-csv/)
2.  [PySpark MLlib](https://www.educba.com/pyspark-mllib/)
3.  [PySpark 数据帧](https://www.educba.com/pyspark-dataframe/)
4.  PySpark fillna





