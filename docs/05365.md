# PySpark 地图分区

> 原文:[https://www.educba.com/pyspark-mappartitions/](https://www.educba.com/pyspark-mappartitions/)

![PySpark mappartitions](../Images/2a499f6a02b9cbe7cd824fa5f7589bd7.png)

<noscript><img class="alignnone size-full wp-image-485922" src="../Images/2a499f6a02b9cbe7cd824fa5f7589bd7.png" alt="PySpark mappartitions" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions.jpg"/></noscript>

## PySpark 映射分区简介

PYSPARK mapPartitions 是一种转换操作，适用于 RDD 中的每个分区。它是 RDD 的一个特性，将函数应用于 RDD 的分区。众所周知，PySpark 中的 RDD 将数据存储在分区中，而 mapPartitions 用于在 PySpark 架构中的 RDD 分区上应用函数。

它只能应用于 PySpark 中的 RDD，因此我们需要将数据框/数据集转换为 RDD，以便对其应用地图分区。在进行映射分区时，没有数据移动或移动。与使用的输入行相比，作为输出返回的行数相同。

<small>网页开发、编程语言、软件测试&其他</small>

在本文中，我们将尝试分析使用 MAPPARTITIONS 操作 PySpark 的各种方法。

让我们试着更详细地了解映射分区。

【PySpark 映射分区的语法

MAPPARTITIONS 函数的语法是:-

`df2=b.rdd.mapPartitions(fun).toDF(["name","ID"])
b:- The Dataframe that is used post converted to RDD
mappartitions:- The MapPartitions to be used on the partition over the RDD partitions.
toDF:- The to Data frame conversion.
Df2:- The Final data frame formed`

**截图:-**

![PySpark mappartitions output 1](../Images/542bcd8b76bd12bd559e1dbb73b3f5a7.png)

<noscript><img class="alignnone size-full wp-image-484764" src="../Images/542bcd8b76bd12bd559e1dbb73b3f5a7.png" alt="PySpark mappartitions output 1" width="468" height="181" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-1.png 468w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-1-300x116.png 300w" sizes="(max-width: 468px) 100vw, 468px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-1.png"/></noscript>

### PySpark 映射分区的工作方式

让我们看看 MAPPARTITIONS 是如何工作的

mapPartitions 是在 PySpark 模型的 RDD 中应用于特定分区的转换。这可以作为 Map()和 foreach()的替代方法。

返回类型与 RDD 中的行数相同。在 MapPartitions 中，该函数应用于 RDD 中的类似分区，从而提高了性能。

需要对每个分区进行一次性调用的数据模型的繁重初始化是通过使用 MapPartitions 来完成的。RDD 将数据存储在分区中，操作应用于每个元素，但在 MapPartitions 中，函数应用于 RDD 数据模型中的每个分区。

到数据库的连接是一个示例，需要在每个分区上应用一次，这有助于进一步分析数据，MapPartitions 非常适合这个模型，并且连接是基于数据的分区进行的。

MapPartitions 将结果保存在内存中，直到分区中的所有行都得到处理。

让我们用一些代码例子来检查 MAPPARTITIONS 的创建和工作。

### PySpark 映射分区示例

让我们看一些 MAPPARTITIONS 操作如何工作的例子

让我们从在 PySpark 中创建简单数据开始。

`>>> data1  = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USA'},{'Name':'Joe','ID':5.33,'Add':'INA'}] A sample data is created with Name , ID and ADD as the field.
>>> a = sc.parallelize(data1)
RDD is created using sc.parallelize.
>>> b = spark.createDataFrame(a)
Created Data Frame using Spark.createDataFrame.`

**截图:-**

![PySpark mappartitions output 2](../Images/1b6577b2165459b4f47b3f4f3978aaf1.png)

<noscript><img class="alignnone size-full wp-image-484765" src="../Images/1b6577b2165459b4f47b3f4f3978aaf1.png" alt="PySpark mappartitions output 2" width="624" height="114" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-2.png 624w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-2-300x55.png 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-2-620x114.png 620w" sizes="(max-width: 624px) 100vw, 624px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-2.png"/></noscript>

让我们创建一个简单的函数，它接受 name 和 ID 并将其传递给 MapPartitions 方法。

这会遍历 rdd 并从中产生名称和 ID。

`def fun(p):
for row in p:
yield [row.Name+",",row.ID] We will convert this data frame into RDD and pass the mapPartitions function inside this. The post method toDF will create the RDD again with the name as the schema.
>>> df2=b.rdd.mapPartitions(fun).toDF(["name","Concated_Name"])`

**截图:-**

![PySpark mappartitions output 3](../Images/e00fc0ef2ec55d89291184f03f2a8740.png)

<noscript><img class="alignnone size-full wp-image-484766" src="../Images/e00fc0ef2ec55d89291184f03f2a8740.png" alt="PySpark mappartitions output 3" width="553" height="271" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-3.png 553w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-3-300x147.png 300w" sizes="(max-width: 553px) 100vw, 553px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-3.png"/></noscript>

让我们来看看 MapPartitions 元素如何处理分区数据。

`>>> rdd = sc.parallelize([1, 2, 3, 4,3,5,2,1,3], 3)
>>> def fun2(iterator): yield sum(iterator)
>>> rdd.mapPartitions(fun2).collect()`

该函数返回数据分区中列出的元素的总和。

**输出:-**

`[6, 12, 6]`

**截图:-**

![PySpark mappartitions output 4](../Images/5f09733ebd6e00145ca0e10fbbc1752f.png)

<noscript><img class="alignnone size-full wp-image-484767" src="../Images/5f09733ebd6e00145ca0e10fbbc1752f.png" alt="PySpark mappartitions output 4" width="494" height="114" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-4.png 494w, https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-4-300x69.png 300w" sizes="(max-width: 494px) 100vw, 494px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/06/PySpark-mappartitions-output-4.png"/></noscript>

这些是映射分区的一些例子。

此功能可用于创建逻辑，这些逻辑可在每个分区应用一次，如连接创建、连接终止。

**注:-**

1.  MAPPARTITIONS 是 PySpark RDD 的一个转换操作模型。
2.  MAPPARTITIONS 应用于 PySpark 中的 RDD，因此数据框需要转换为 RDD。
3.  映射分区应用于需要大量转换的逻辑或功能。
4.  MAPPARTITIONS 应用于模型中的特定分区，而不是 PySpark 中的每个行模型。
5.  MAPPARTITIONS 将结果保存在分区内存中。
6.  MAPPARTITIONS 是一种更快、更便宜的数据处理模型。

### 结论

从上面的文章中，我们看到了映射分区的工作原理。通过各种例子和分类，我们试图理解这个 MAPPARTITIONS 函数是如何工作的，以及在编程级别使用了什么。所使用的各种方法显示了它如何简化数据分析的模式以及同样的成本效益模型。

我们还看到了 MAPPARTITIONS 在 Data Frame 中的内部工作和优势，以及它在各种编程目的中的使用。此外，语法和例子帮助我们更准确地理解函数。

### 推荐文章

这是一个 PySpark 地图分区指南。这里我们讨论 PySpark 数据框架中 MAPPARTITIONS 的内部工作原理和优点。您也可以看看以下文章，了解更多信息–

1.  [PySpark 结构类型](https://www.educba.com/pyspark-structtype/)
2.  [PySpark 窗口函数](https://www.educba.com/pyspark-window-functions/)
3.  [PySpark 列到列表](https://www.educba.com/pyspark-column-to-list/)
4.  [PySpark 回合](https://www.educba.com/pyspark-round/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>