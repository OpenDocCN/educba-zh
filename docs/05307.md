# 火花 RDD 操作

> 原文:[https://www.educba.com/spark-rdd-operations/](https://www.educba.com/spark-rdd-operations/)

![Spark RDD Operations](../Images/8d54df77eff5b9f7cb1e9951eb85f884.png)

<noscript><img class="alignnone size-full wp-image-443955" src="../Images/8d54df77eff5b9f7cb1e9951eb85f884.png" alt="Spark RDD Operations" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-9.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-9-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-9-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-9.jpg"/></noscript>

## 星火 RDD 运营简介

Spark RDDs 支持两种类型的操作:

*   **转换:**转换是通过修改现有的 RDD/rdd 返回新的 RDD 的函数。输入 RDD 不被修改，因为 rdd 是不可变的。
*   **动作:**对输入数据进行一定的计算后，将结果返回给驱动程序(或将数据存储到 hdfs 之类的外部存储器中)。

Spark 以一种懒惰的方式执行所有的转换。结果没有马上计算出来。只有在 RDD 上执行某个操作时，才会进行变换计算。

<small>Hadoop、数据科学、统计学&其他</small>

### 转换和操作

下面给出了转换和操作:

#### 1.转换

它们大致分为两种类型:

**缩小变换:**

*   计算一个分区中的记录所需的所有数据都驻留在父 RDD 的一个分区中。
*   它发生在下列方法的情况下:

map()、flatMap()、filter()、sample()、union()等。

**宽变换:**

*   计算一个分区中的记录所需的所有数据都驻留在父 rdd 的多个分区中。
*   它发生在下列方法的情况下:

distinct()、groupByKey()、reduceByKey()、join()、repartition()等。

#### 2.行动

转换只是从另一个 RDD/RDDs 中创建一个 RDD，但是当我们想要看到任何结果时，我们必须对它调用一个动作。当在任何 RDD 上调用一个动作时，结果不是一个 RDD，而是一些结果被扔给驱动程序，或者一些数据被存储在一些外部存储器中。因此，动作是从执行者向驱动者发回一些结果的一种方式。

一些常见的执行者操作有:

count()、collect()、take()、reduce()、saveAsTextFile()等。

### 火花 RDD 操作的例子

以下是火花 RDD 操作的例子:

**转换:**

#### 示例 1 地图()

该函数将一个函数作为参数，并将该函数应用于 RDD 的每个元素。

**代码:**

`val conf = new SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd = sc.parallelize(Array(10,15,50,100))
println("Base RDD is:")
rdd.foreach(x => print(x+" "))
println()
val rddNew = rdd.map(x => x+10)
println("RDD after applying MAP method:")
rddNew.foreach(x => print(x+" "))`

**输出:**

![Spark RDD Operations 1](../Images/1d8f7311c262576df2ced41ba69c3dd1.png)

<noscript><img class="alignnone wp-image-443766 size-full" src="../Images/1d8f7311c262576df2ced41ba69c3dd1.png" alt="Spark RDD Operations 1" width="438" height="116" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-1.jpg 438w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-1-300x79.jpg 300w" sizes="(max-width: 438px) 100vw, 438px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-1.jpg"/></noscript>

在上面的 MAP 方法中，我们将每个元素加 10，这反映在输出中。

#### 示例 2:过滤器()

它将一个函数作为参数，并返回该函数返回 true 的 RDD 的所有元素。

**代码:**

`val conf = new SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd = sc.parallelize(Array("com.whatsapp.prod","com.facebook.prod","com.instagram.prod","com.whatsapp.test"))
println("Base RDD is:")
rdd.foreach(x => print(x+" "))
println()
val rddNew = rdd.filter (x => !x.contains("test"))
println("RDD after applying MAP method:")
rddNew.foreach(x => print(x+" "))`

**输出:**

![Spark RDD Operations 2](../Images/71443f59b52d29b656f805ffe67a046e.png)

<noscript><img class="alignnone wp-image-443771 size-full" src="../Images/71443f59b52d29b656f805ffe67a046e.png" alt="Spark RDD Operations 2" width="931" height="136" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-2.jpg 931w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-2-300x44.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-2-768x112.jpg 768w" sizes="(max-width: 931px) 100vw, 931px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-2.jpg"/></noscript>

在上面的代码中，我们取的是没有单词“test”的字符串。

#### 示例 3: distinct()

此方法返回 RDD 的不同元素。

**代码:**

`val conf = new
SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd = sc.parallelize(Array(1,1,3,4,5,5,5))
println("Base RDD is:")
rdd.foreach(x => print(x+" "))
println()
val rddNew = rdd.distinct()
println("RDD after applying MAP method:")
rddNew.foreach(x => print(x+" "))`

**输出:**

![Spark RDD Operations 3](../Images/bd2168e6e0e7fb55beb483074ab6952d.png)

<noscript><img class="alignnone wp-image-443772 size-full" src="../Images/bd2168e6e0e7fb55beb483074ab6952d.png" alt="Spark RDD Operations 3" width="409" height="133" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-3.jpg 409w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-3-300x98.jpg 300w" sizes="(max-width: 409px) 100vw, 409px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-3.jpg"/></noscript>

我们在输出中得到不同的元素 4，1，3，5。

#### 示例 4: join()

join 操作适用于成对 rdd。成对 RDD 的每个元素都是一个元组，其中第一个元素是键，第二个元素是值。join 方法根据键组合两个数据集。

**代码:**

`val conf = new SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd1 = sc.parallelize(Array(("key1",10),("key2",15),("key3",100)))
val rdd2 = sc.parallelize(Array(("key2",11),("key2",20),("key1",75)))
val rddJoined = rdd1.join(rdd2)
println("RDD after join:")
rddJoined.foreach(x => print(x+" "))`

**输出:**

![Spark RDD Operations 4](../Images/b0fa58ab094c54d91e41cfb9870bde57.png)

<noscript><img class="alignnone wp-image-443773 size-full" src="../Images/b0fa58ab094c54d91e41cfb9870bde57.png" alt="Spark RDD Operations 4" width="609" height="82" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-4.jpg 609w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-4-300x40.jpg 300w" sizes="(max-width: 609px) 100vw, 609px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-4.jpg"/></noscript>

**动作:**

#### 示例 5:计数()

该函数给出了 RDD 中元素的数量。

**代码:**

`val conf = new
SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10))
val cnt= rdd.count()
println("The count is "+cnt)`

**输出:**

![Spark RDD Operations 5](../Images/9fae30ed67b53df50b64af7eb5ddf989.png)

<noscript><img class="alignnone wp-image-443775 size-full" src="../Images/9fae30ed67b53df50b64af7eb5ddf989.png" alt="Spark RDD Operations 5" width="327" height="59" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-5.jpg 327w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-5-300x54.jpg 300w" sizes="(max-width: 327px) 100vw, 327px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-5.jpg"/></noscript>

不出所料，我们得到了 RDD 中元素的数量。

#### 示例 6: collect()

该方法将 RDD 中的所有元素作为数组提供给驱动程序。这种方法的限制是所有数据必须适合驱动程序。这种方法通常在编写测试用例时使用，在这种情况下，元素的预期数量较小，并且事先已知。

**代码:**

`val conf = new
SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10))
val arr= rdd.collect()
println("Printing the elements in the array")
arr.foreach(x => print(x+" "))`

**输出:**

![collect()](../Images/d842fb3e950f4be513a282695e0bc6dd.png)

<noscript><img class="alignnone wp-image-443777 size-full" src="../Images/d842fb3e950f4be513a282695e0bc6dd.png" alt="collect()" width="450" height="76" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-6.jpg 450w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-6-300x51.jpg 300w" sizes="(max-width: 450px) 100vw, 450px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-6.jpg"/></noscript>

在上面的代码中，我们将 RDD 中的所有元素提取到数组“arr”中，然后打印每个元素。

#### 示例 7: countByKey()

该功能适用于成对 rdd。我们之前讨论过什么是成对 rdd。它返回包含每个键的计数的哈希映射。

**代码:**

`val conf = new SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")
val rdd = sc.parallelize(Array(("a",10),("a",5),("b",1),("b",4),("a",5),("c",20)))
val map= rdd.countByKey()`

**输出:**

![countByKey()](../Images/17220c28d150e4998ede5ddcc99c457c.png)

<noscript><img class="alignnone wp-image-443779 size-full" src="../Images/17220c28d150e4998ede5ddcc99c457c.png" alt="countByKey()" width="472" height="69" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-7.jpg 472w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-7-300x44.jpg 300w" sizes="(max-width: 472px) 100vw, 472px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-7.jpg"/></noscript>

在上面的例子中，有 3 个键 a、b 和 c，在输出中，我们得到每个键在输入中出现的次数。

#### 示例 8: reduce()

该函数将另一个函数作为参数，该参数依次一次接受 RDD 的两个元素并返回一个元素。这用于聚合。

**代码:**

`val conf = new
SparkConf().setMaster("local").setAppName("testApp")
val sc= SparkContext.getOrCreate(conf)
sc.setLogLevel("ERROR")`

**输出:**

![reduce()](../Images/19d9280b8c71f5fc6b6ca876dd0ca210.png)

<noscript><img class="alignnone wp-image-443781 size-full" src="../Images/19d9280b8c71f5fc6b6ca876dd0ca210.png" alt="reduce()" width="431" height="45" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-8.jpg 431w, https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-8-300x31.jpg 300w" sizes="(max-width: 431px) 100vw, 431px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/12/Spark-RDD-Operations-8.jpg"/></noscript>

在上面的代码中，我们传递了一个 reduce 函数，它将接收到的两个元素相加，因此在输出中我们得到了输入中所有元素的总和。

### 结论

在本文中，我们看到了如何使用转换来产生不同的 rdd。我们也看到了转换在本质上是懒惰的，只有当一些动作被调用时才被计算。我们还看到了在 rdd 上频繁执行的各种操作。

### 推荐文章

这是一份 RDD 星火行动指南。这里我们分别讨论了引入、转换和动作、例子。您也可以看看以下文章，了解更多信息–

1.  [火花上下文](https://www.educba.com/sparkcontext/)
2.  [火花簇](https://www.educba.com/spark-cluster/)
3.  [火花纱](https://www.educba.com/spark-yarn/)
4.  [火花累加器](https://www.educba.com/spark-accumulator/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>