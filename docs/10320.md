# Kubernetes GPU

> 原文:[https://www.educba.com/kubernetes-gpu/](https://www.educba.com/kubernetes-gpu/)

![kubernetes gpu](../Images/91af2f168cf0268a8c6c920025de22d8.png)

<noscript><img class="alignnone size-full wp-image-513164" src="../Images/91af2f168cf0268a8c6c920025de22d8.png" alt="kubernetes gpu" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/10/kubernetes-gpu.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2021/10/kubernetes-gpu-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/10/kubernetes-gpu-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/10/kubernetes-gpu.jpg"/></noscript>

## gpu 组合简介

在这里，GPU 允许组织或企业将其部署和培训扩展到多云 GPU 集群。在 GPU 的帮助下，我们可以轻松地自动化我们的部署过程、调度、维护，多个 GPU 的操作也有助于我们在整个集群的节点上加速应用程序的容器。正如我们所看到的，我们有越来越多的人工智能服务和应用程序，以及公共云中大量可用的 GPU，因此我们需要像 Kubernetes 这样的开源软件能够强烈地感知 GPU。在本教程的下一部分，我们将详细地看到它的内部工作、实现和其他重要的东西，以便更清楚地使用它，让初学者更好地理解它。

### 什么是 Kubernetes gpu？

正如我们所看到的，GPU 允许企业自动化其部署、操作、维护、调度等过程。;此外，它还有助于加速节点集群中的应用程序容器。此外，Kubernetes 支持和维护 GPU，它代表集群中各个节点的图形处理单元。为了使用 GPU，我们已经安装了来自适当供应商的软件，您可以根据您的要求进行选择。正如我们所看到的，我们在 AI/ML 领域有很大的用途；这也导致了 GPU 的增长，因为它维护和支持计算机模型来训练模型，处理图像和许多其他东西。让我们来看看关于 Kubernetes GPUs 的几点，以便更好地理解它。

<small>网页开发、编程语言、软件测试&其他</small>

1)简而言之，Kubernetes 不具备我们所说的处理 GPU 资源的能力或支持。

2)因此它为他们提供了扩展的资源。

3)我们有不同的供应商为 Kubernetes 中的 GPU 扩展资源提供支持和维护

4)我们必须先安装这个软件。

### 在 Kubernetes 和 Kubernetes GPU 设置中使用 GPU

在本节中，我们将仔细研究 Kubernete 中 GPU 的使用情况；为此，我们要做几个配置；之后，我们可以使用它。首先，我们将设置 Kubernete，第二个是我们的 Ray 集群配置。为了更清楚起见，我们先详细了解一下射线束；我们有 Ray Docker Hub，它帮助我们托管与 Ray 打包在一起的 CUDA 图像，以便与 Kubernete pods 一起使用。

现在我们有 Nvidia GPUs 可以用了；我们还必须做一些配置，比如我们必须在 Kubernete 配置中指定相关的资源。因此，我们有一个标准文件，显示我们有一个运行 Ray GPU 和使用 Nvidia GPU 的 pod 该文件应该如下所示；

**例如:**

`apiVersion: v1
kind: Pod
metadata:
generateName: example-cluster-ray-worker
spec:
...
containers:
- name: ray-node
image: rayproject/ray:nightly-gpu
...
resources:
cpu: 1000m
memory: 512Mi
limits:
memory: 512Mi
nvidia.com/gpu: 1`

由于该字段显示了基本配置，我们已经提到了 apiVersion、图像、内存和 nvidia GPU。这一切我们在使用 nvidia GPU 的时候都可以提到；这是您可以用来配置 GPU 设置的基本文件。现在我们将详细讨论 nvidia GPUs 的容错和训练；

1)我们有 Nvidia GPU 插件，它帮助我们将 trains 应用到 Kubernete 的 GPU 节点；这一系列的帮助在于:它允许我们防止非 gpu 节点在 GPU 节点上被调度使用。

2)我们还有 Kubernete 托管服务，如 EKS、AKS 和 GKE；这些都会自动对请求 GPU 资源的 pod 应用容忍。

3)这种容忍是通过使用 ETAC 来应用的，它代表扩展资源容忍接纳控制器。

4)我们必须在 Kubernete 集群上启用这个控制器；如果没有，我们可能需要手动将容错添加到 GPU 容错和每个 GPU pod 配置中。

5)对于这一点，我们有一个显示标准配置的文件；见下文；

**例如:**

`apiVersion: v1
kind: Pod
metadata:
generateName: example-cluster-ray-worker
spec:
...
tolerations:
- effect: NoSchedule
key: nvidia.com/gpu
operator: Exists
...
containers:
- name: ray-node
image: rayproject/ray:nightly-gpu
...`

### 搜索 GPU 配额

因为我们知道它是一个扩展资源，所以在 1.10 的版本中，他们增加了扩展资源。还不允许过量使用扩展资源，因为指定这两者(即 GPU 配额中的扩展资源的请求和限制)没有任何意义。到目前为止，只有它允许带有前缀 request 的配额项目。在本节中，我们将了解如何通过使用以下配置来限制 nvidia 配额资源；此外，我们将了解它正在做什么，并指定见下文；

**例如:**

`requests.nvidia.com/gpu: 10`

在上面的代码行中，资源名称是“nvidia.com/gpu”，这里我们试图将 gpu 请求使用的扩展资源的总数限制为 10。如果需要，我们可以自己进行配置，只需修改即可。

### Kubernetes gpu 共享

在本节中，我们将讨论 GPU 的共享，以下是几个要点；

1)我们能够指定 GPU 的限制，甚至无需指定请求。

2)我们也可以指定请求和限制，但两个值应该相等。

3)此外，还有一个标准是，我们不能在没有指定限制的情况下指定请求。

4)这意味着 pod 和容器不共享 GPU。

但是 GPU 的共享至少在 Nvidia GPUs 的情况下可以做到。因此，对于这一点，只是不要指定两个值，即限制和要求。

### 结论

在这个 GPU 教程中，我们看到了所有与 Kubernetes GPU 相关的东西，如何使用和实现它。跟随整篇文章，以获得更好的清晰感和对细节的理解。参考配置文件也在那里开始您的配置。

### 推荐文章

这是一个 Kubernetes GPU 的指南。在这里我们讨论所有与 Kubernetes GPU 相关的东西，如何使用和实现它。您也可以看看以下文章，了解更多信息–

1.  [Kubernetes 主机路径](https://www.educba.com/kubernetes-hostpath/)
2.  [Kubernetes 自动缩放](https://www.educba.com/kubernetes-autoscaling/)
3.  [Kubernetes 选择器](https://www.educba.com/kubernetes-selector/)
4.  [立方〔t1〕](https://www.educba.com/kubernetes-kubectl/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>