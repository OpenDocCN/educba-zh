# pykick wit 列

> 原文：<https://www.educba.com/pyspark-withcolumn/>

![PySpark withColumn](img/1f0606e6905ffc10a9644470632032e9.png)



## PySpark withColumn 简介

PySpark withColumn 是 PySpark 中的一个函数，主要用于转换具有各种所需值的数据帧。转换可能意味着改变值、转换列的数据类型或添加新列。PySpark 中的所有这些操作都可以通过使用 With Column 操作来完成。

with Column 操作适用于选定的行或所有行的列值。这将在执行操作后返回新的数据帧。它是一个转换函数，只在 PySpark 数据帧上执行动作后调用。

<small>网页开发、编程语言、软件测试&其他</small>

【PySpark withColumn 的语法:

PySpark withColumn 函数的语法是:

`from pyspark.sql.functions import current_date
b.withColumn("New_date", current_date().cast("string"))`

b:--py spark 数据帧。

with column:-要处理的 with column 函数。

`“New_Date”:- The new column to be introduced.
current_date().cast("string")) :- Expression Needed.`

**截图:**

![PySpark withColumn 1](img/cde8d207da2612cde000073498774112.png)



### 在 PySpark 中使用列

让我们看看 PySpark 中的 WITHCOLUMN 函数是如何工作的:

With Column 函数转换数据并添加新的列。它在数据框中添加新列，并从同一数据框中添加更新后的值。这个更新后的列可以是新的列值，也可以是具有已更改实例(如数据类型或值)的旧列值。它返回一个新的数据帧，旧的数据帧被保留。它在内部引入了一个投影。制定并执行计划，并对计划进行所需的转换。操作，如添加列、更改现有列的现有值、从旧列派生新列、更改数据类型、添加和更新列、重命名列，都是在 with column 的帮助下完成的。我们还可以使用 with column 删除列，并根据该列创建新的数据框。

### 例子

让我们看一些 PySpark 与 Column 函数如何工作例子:

让我们从在 PySpark 中创建简单数据开始。

`data1 = [{'Name':'Jhon','ID':2,'Add':'USA'},{'Name':'Joe','ID':3,'Add':'USA'},{'Name':'Tina','ID':2,'Add':'IND'}]`

创建一个示例数据，字段为 Name、ID 和 ADD。

`a = sc.parallelize(data1)
RDD is created using sc.parallelize.
b = spark.createDataFrame(a)
b.show()`

使用 Spark.createDataFrame 创建了 DataFrame。

**截图:**

![PySpark withColumn 2](img/187f72d18ebbebae9049ce7a462c756c.png)



#### 1.更改数据框中现有列的数据类型。

让我们尝试更改列的数据类型，并在 PySpark 数据框中使用 with column 函数。

**代码:**

`from pyspark.sql.functions import col
b.withColumn("ID",col("ID").cast("Integer")).show()`

**输出:**

这会将列数据类型转换为整数。

**截图:**

**![PySpark withColumn 3](img/d6e4663779b10bb23cd5183de7dd2224.png)

** 

#### 2.更新数据框现有列的值。

让我们尝试更新一个列的值，并在 PySpark 数据框中使用 with column 函数。

**代码:**

`from pyspark.sql.functions import col
b.withColumn("ID",col("ID")+5).show()`

**输出:**

这将更新数据框的列并为其添加值。

**截图:**

![Output 4](img/596a9b256c6806b8ff61a37155a3d0b8.png)



#### 3.在数据框中创建新列。

**代码:**

`from pyspark.sql.functions import col
b.withColumn("New_Column",col("ID")+5).show()`

**输出:**

这将创建一个新列并为其赋值。

**截图:**

![Output 5](img/853324e81be7043f97bcce5baadd020f.png)



#### 4.使用 With Column 添加到数据框中的列。

**代码:**

`from pyspark.sql.functions import col, lit
b.withColumn("New_Column",lit("NEW")).show()`

**输出:**

这将使用 LIT 函数添加一个具有常数值的新列。

**截图:**

![Output 6](img/df987d2f98b5808a996cca5d619633e1.png)



#### 5.添加多列。

**代码:**

`from pyspark.sql.functions import col
b.withColumn("New_Column",lit("NEW")).withColumn("New_Column2",col("Add")).show()`

**输出:**

这将 PySpark 数据框中的多列相加。我们可以在一个数据框中添加多个列，并在其中实现值。

**截图:**

![Output 7](img/4d7f0e6fec9eeb5eb0fa110c010ea02d.png)



#### 6.重命名现有列。

with column renamed 函数用于重命名 Spark 数据框中的现有函数。

**代码:**

`from pyspark.sql.functions import col
b.withColumnRenamed("Add","Address").show()`

**输出:**

这将重命名 PYSPARK 中现有数据框中的一列。这些是 PySpark 中 WITHCOLUMN 函数的一些例子。

**注:**

1.With Column 用于处理数据框中的列。
2。With Column 可用于在数据框上创建变换。
3。这是一个转换函数。
4。它接受两个参数。我们要处理的列名和新列。

### 结论

从上面的文章中，我们看到了 PySpark 中 WithColumn 操作的使用。从各种示例和分类中，我们试图理解 WITHCOLUMN 方法在 PySpark 中是如何工作的，以及在编程级别中使用了什么。
我们还看到了 Spark 数据帧中 WithColumn 的内部工作和优势，以及它在各种编程目的中的使用。此外，语法和例子帮助我们更精确地理解了函数。

### 推荐文章

这是 PySpark withColumn 的指南。在这里，我们讨论的介绍，语法，代码实现的例子。您也可以看看以下文章，了解更多信息–

1.  [PySpark 滤波器](https://www.educba.com/pyspark-filter/)
2.  [PySpark 版本](https://www.educba.com/pyspark-version/)
3.  [PySpark 加入](https://www.educba.com/pyspark-join/)
4.  [PySpark SQL](https://www.educba.com/pyspark-sql/)





