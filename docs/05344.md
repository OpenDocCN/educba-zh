# PySpark 地图

> 原文:[https://www.educba.com/pyspark-map/](https://www.educba.com/pyspark-map/)

![PySpark Map](../Images/378a01ef9bed5f29f1cb12c1ad1ccaae.png)

<noscript><img class="alignnone size-full wp-image-461343" src="../Images/378a01ef9bed5f29f1cb12c1ad1ccaae.png" alt="PySpark Map" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/04/PySpark-Map.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2021/04/PySpark-Map-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/04/PySpark-Map-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/04/PySpark-Map.jpg"/></noscript>

## PySpark 地图简介

PySpark 地图是 PySpark 中的一种变换，应用于 Spark 应用程序中 RDD /数据框的每个函数。返回类型是应用了地图函数的新 RDD 或数据框。它用于对 PySpark 应用程序中的每个元素进行操作，如转换、列更新等。

地图操作是一个简单的 spark 变换，它占用数据框/ RDD 的一个元素，并对其应用给定的变换逻辑。我们可以定义我们自己的自定义转换逻辑或从库中导出的函数，并使用 map 函数来应用它。返回的结果将是一个新的 RDD，其元素数量与旧的相同。

<small>网页开发、编程语言、软件测试&其他</small>

**语法:**

PYSPARK 映射函数的语法是:

`a.map (lambda x : x+1)`

**截图:**

![PySpark Map 1-1](../Images/14ebd44da5cafe2f5eba8cafe2fcb15c.png)

<noscript><img class="alignnone size-full wp-image-461166" src="../Images/14ebd44da5cafe2f5eba8cafe2fcb15c.png" alt="PySpark Map 1-1" width="367" height="80" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-1.png 367w, https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-1-300x65.png 300w" sizes="(max-width: 367px) 100vw, 367px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-1.png"/></noscript>

**说明:**

*   答:数据框或 RDD。
*   映射:要应用的映射变换。
*   Lambda:要应用的函数。

### PySpark 中的地图工作

让我们看看 PySpark 中的 MAP 函数是如何工作的

地图变换适用于 PySpark 中 RDD /数据框的每个元素。这将长度为 L 的 RDD 转换成另一个长度为 L 的应用了逻辑的长度。因此输入和输出将具有与预期相同的记录。

转换基本上是遍历所有元素，并应用在函数上给出的逻辑。将逻辑应用于每个元素后，结果将作为 PySpark 中的新 RDD /数据帧返回。我们可以将自己的自定义逻辑应用于元素，但需要明确的是，该逻辑应用于 RDD 或数据框中的每个元素。

### 例子

让我们看一些 PYSPARK 映射函数如何工作的例子:

让我们首先创建一个 PySpark RDD。

一个非常简单的方法是使用 sc。并行化功能。

`a = sc.parallelize([1,2,3,4,5,6])`

这将创建一个 RDD，在这里我们可以应用映射函数来定义自定义逻辑。

让我们尝试定义一个简单的函数，将 RDD 中的每个元素加 1，并通过 Map 函数将它传递给 PySpark 应用程序中的每个 RDD。

我们将开始为加法编写一个 lambda 函数，并通过 RDD 将它传递到 Map 函数中。

`b= a.map(lambda x : x+1)`

这将为 RDD 中的每个元素加 1，结果将存储在一个新的 RDD 中。

RDD.collect 方法将用于进一步收集和定义结果。

`b.collect()`

**输出:**

![Output 1-2](../Images/64915b9976eea9565c993d9a0f205b06.png)

<noscript><img class="alignnone wp-image-461167 size-full" src="../Images/64915b9976eea9565c993d9a0f205b06.png" alt="Output 1-2" width="576" height="201" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-2.png 576w, https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-2-300x105.png 300w" sizes="(max-width: 576px) 100vw, 576px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-2.png"/></noscript>

这个简单的例子展示了如何将 map 操作应用于 PySpark RDD 中的所有元素。

让我们通过使用一个复杂的 RDD 函数来检查一个使用 map 函数计算 PySpark RDD 单词数的例子。

我们将从制作 PySpark RDD 开始。

`a = sc.parallelize(["Bob","Sam","Peter","Mona","SHyam","Bob"])`

任务是应用 Map 函数，并计算其中每个单词的出现次数。

首先，我们将为每个元素分配一个值，然后在 RDD 上使用“按键减少”操作对值进行后期分配添加。

我们将使用 add 函数从 operator 导入，用相同的键对 RDD 执行 add 操作。

`from operator import add
b = a.map(lambda x:(x,1)).reduceByKey(add).collect()
for (w, c) in b:
print("{}: {}".format(w, c))`

这将打印出 PySpark RDD 中所有单词出现的输出。

**截图:**

![Output 1-3](../Images/66489ceba5daa655c3cc735b0de2054a.png)

<noscript><img class="alignnone wp-image-461169 size-full" src="../Images/66489ceba5daa655c3cc735b0de2054a.png" alt="Output 1-3" width="543" height="241" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-3.png 543w, https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-3-300x133.png 300w" sizes="(max-width: 543px) 100vw, 543px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/03/PySpark-Map-1-3.png"/></noscript>

我们还可以在 RDD 上使用 sort by 关键字对地图操作后收集的元素进行排序。

`sort = b.sortBy(lambda x : x[1] , ascending = False)
sort.collect`

这将输出排序后的 RDD。

**截图:**

![4-1](../Images/4b1ac0fc81592565c254a4ab2f5766f6.png)

<noscript><img class="alignnone size-full wp-image-461170" src="../Images/4b1ac0fc81592565c254a4ab2f5766f6.png" alt="4-1" width="604" height="91" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/03/4-1.png 604w, https://cdn.educba.com/academy/wp-content/uploads/2021/03/4-1-300x45.png 300w" sizes="(max-width: 604px) 100vw, 604px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/03/4-1.png"/></noscript>

从上面的例子中，我们看到了在 PySpark 中使用 ForEach 函数

### 结论

从上面的文章中，我们看到了 PySpark 中 MAP 的使用。从各种例子和分类中，我们试图理解 MAP 方法在 PySpark 中是如何工作的，以及在编程级别使用了什么。工作模式使我们正确理解了职能的内涵，并帮助我们获得了更多的相关知识。语法有助于检查使用的确切参数和函数的功能知识。

我们还看到了 Spark Data Frame 中的 MAP inPySpark 的内部工作和优势，以及它在各种编程目的中的使用。此外，语法和例子帮助我们更准确地理解函数。

### 推荐文章

这是 PySpark 地图指南。这里我们讨论 PySpark 中 Map 的介绍、工作原理以及代码实现的例子。您也可以看看以下文章，了解更多信息–

1.  [PySpark 加入](https://www.educba.com/pyspark-join/)
2.  [PySpark SQL](https://www.educba.com/pyspark-sql/)
3.  [火花上下文](https://www.educba.com/sparkcontext/)
4.  [火花簇](https://www.educba.com/spark-cluster/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>