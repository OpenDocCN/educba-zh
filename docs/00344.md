# PySpark GitHub

> 原文:[https://www.educba.com/pyspark-github/](https://www.educba.com/pyspark-github/)

![PySpark GitHub](../Images/38c422beb8860b1af20b8a4bd9ad338c.png)

<noscript><img class="alignnone size-full wp-image-553476" src="../Images/38c422beb8860b1af20b8a4bd9ad338c.png" alt="PySpark GitHub" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub.jpg"/></noscript>

## PySpark GitHub 的定义

PySpark GitHub 是 PySpark 中的一个进程，我们在指定的时间内使用它。为了创建 PySpark GitHub，用户使用 apache 提交 Spark 应用程序。驱动模块用于 GitHub 项目，该项目从 spark 端获取应用程序。PySpark 驱动程序将从 Spark 端识别应用程序。按照程序驱动程序，它将识别应用程序中存在的转换。

### PySpark GitHub 是什么？

PySpark GitHub 的 Python 模块将包含 ETL 作业的 apache spark 定义，它实现了我们在生产中运行的 ETL 作业的最佳实践。我们可以通过使用 smart submit 命令使用 spark 集群来提交相同的内容，这个命令与我们在 spark 发行版的 bin 目录中找到的命令相同。zip 包将包含 ETL 作业所需的 python 模块。GitHub 在 executor 的每个进程和集群中的每个模式中都是可用的。

<small>网页开发、编程语言、软件测试&其他</small>

我们需要将配置文件发送到包含 json 对象的集群，该对象包含在 GitHub 上运行项目所需的所有配置参数。PySpark GitHub 项目的文件包含 Spark 的应用程序，该程序在 spark master 节点的驱动程序进程中执行。为了使用 PySpark GitHub，我们需要在系统中安装 git bash。我们可以通过从 g it 网站下载来安装它。我们还可以使用 py charm 来设置 GitHub 项目。

在进行 PySpark 项目时，我们需要将 PySpark 的模块导入到我们的项目中。我们可以使用 import 关键字导入 PySpark 模型。此外，我们需要导入 GitHub 项目所需的库。

### PySpark GitHub 项目

我们可以使用 git 命令下载 GitHub 项目。在下面的项目中，我们从 Kaggle 下载数据集，在那里我们可以下载事务的数据集。我们正在定义网上银行分析。在开发项目时，我们还需要下载数据集。下载完数据集后，我们需要清理数据。

**代码:**

```
git clone */pyspark-project.git
```

**输出:**

![PySpark GitHub Dataset](../Images/34992fbd6111ad57c1773e0708ab46c6.png)

<noscript><img class="alignnone wp-image-553449 size-full" src="../Images/34992fbd6111ad57c1773e0708ab46c6.png" alt="PySpark GitHub Dataset" width="623" height="101" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-1.png 623w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-1-300x49.png 300w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-1-620x101.png 620w" sizes="(max-width: 623px) 100vw, 623px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-1.png"/></noscript>

下载数据集后，我们将检查下载的项目文件，如下所示。我们还可以使用 PySpark 来执行我们已经下载的数据集中的新案例。基本上，PySpark 是一个用于处理大型数据集的框架，如下所示。

**代码:**

```
cd pyspark-project/
```

**输出:**

![PySpark GitHub - Project Files](../Images/3f57800f587851d55d0ec516d2be4221.png)

<noscript><img class="alignnone wp-image-553450 size-full" src="../Images/3f57800f587851d55d0ec516d2be4221.png" alt="PySpark GitHub - Project Files" width="624" height="178" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-2.png 624w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-2-300x86.png 300w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-2-620x178.png 620w" sizes="(max-width: 624px) 100vw, 624px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-2.png"/></noscript>

我们还可以检查项目文件的数据，我们检索数据文件如下。

![PySpark GitHub - Retrieving Data](../Images/77ebdf42e3e224a10c4fcae47e308412.png)

<noscript><img class="alignnone wp-image-553451 size-full" src="../Images/77ebdf42e3e224a10c4fcae47e308412.png" alt="PySpark GitHub - Retrieving Data" width="626" height="310" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-3.png 626w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-3-300x149.png 300w" sizes="(max-width: 626px) 100vw, 626px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-3.png"/></noscript>

PySpark 是执行分析和构建机器学习管道的好语言，它也用于创建数据平台 ETL。当有人在处理大数据时，PySpark 在任何地方都可以使用。它用于处理大量数据，然后分发数据以管理大规模任务，从而获得业务洞察力。

在克隆了下面例子中的项目后，我们通过对数据的操作来定义顺序。

**代码:**

```
import PySpark
from PySpark. Sql import Spark Session
gith = SparkSession.builder.appName("pysparkdf").getOrCreate()
func = gith.read.option ("header", "true").csv("spark2.csv")
func.orderBy ("AUTHOR_ID").show()
```

**输出:**

![PySpark GitHub 4](../Images/40c567d2d0cbefdfefed6e1a9335b832.png)

<noscript><img class="alignnone wp-image-553452 size-full" src="../Images/40c567d2d0cbefdfefed6e1a9335b832.png" alt="PySpark GitHub 4" width="624" height="310" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-4.png 624w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-4-300x149.png 300w" sizes="(max-width: 624px) 100vw, 624px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/09/PySpark-GitHub-4.png"/></noscript>

### PySpark GitHub 函数

PySpark GitHub 在 python 中有多个函数。我们用于数据帧的函数也是一样的。要使用 PySpark GitHub 函数，我们首先需要导入 PySpark 的包，如下所示。

**代码:**

```
import pyspark
from pyspark.sql import SparkSession
```

**输出:**

![Import the Package](../Images/52613dc45859d0efd80893bc32f5f28c.png)

<noscript><img class="alignnone size-full wp-image-568147" src="../Images/52613dc45859d0efd80893bc32f5f28c.png" alt="Import the Package" width="752" height="77" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Import-the-Package.png 752w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Import-the-Package-300x31.png 300w" sizes="(max-width: 752px) 100vw, 752px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Import-the-Package.png"/></noscript>

在下面的例子中，我们使用 show 函数显示 gitlab csv 文件中的数据，如下所示。

**代码:**

```
func = spark.read.option("header", "true").csv("spark1.csv")
func.show()
```

**输出:**

![Show Function](../Images/70c38ea0ca7dce1175e84b41d9314c63.png)

<noscript><img class="alignnone size-full wp-image-568150" src="../Images/70c38ea0ca7dce1175e84b41d9314c63.png" alt="Show Function" width="750" height="352" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Show-Function.png 750w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Show-Function-300x141.png 300w" sizes="(max-width: 750px) 100vw, 750px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Show-Function.png"/></noscript>

我们可以使用打印模式功能打印模式。在下面的示例中，我们可以看到它将显示如下两列。

**代码:**

```
func = spark.read.option("header", "true").csv("spark1.csv")
func.printSchema()
```

**输出:**

![Two Columns](../Images/2ad6e8edd50e468d141a766efc7f9dd7.png)

<noscript><img class="alignnone size-full wp-image-568156" src="../Images/2ad6e8edd50e468d141a766efc7f9dd7.png" alt="Two Columns" width="752" height="152" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Two-Columns.png 752w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Two-Columns-300x61.png 300w" sizes="(max-width: 752px) 100vw, 752px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Two-Columns.png"/></noscript>

以下示例显示了 select 函数。select 函数帮助我们显示从所有数据框中选择的列，我们必须如下传递列名。

**代码:**

```
func = spark.read.option("header", "true").csv("spark1.csv")
func.select('AUTHOR_ID').show(10)
```

**输出:**

![Column Name](../Images/61ecb7a2ed57f6f288bceb5767960304.png)

<noscript><img class="alignnone size-full wp-image-568158" src="../Images/61ecb7a2ed57f6f288bceb5767960304.png" alt="Column Name" width="753" height="258" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Column-Name.png 753w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Column-Name-300x103.png 300w" sizes="(max-width: 753px) 100vw, 753px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Column-Name.png"/></noscript>

pyspark group by 函数用于从数据框中收集数据，并允许我们对已分组的数据执行聚合函数。这是 group by 子句中的常见操作。

**代码:**

```
func = spark.read.option("header", "true").csv("spark1.csv")
func.groupBy("AUTHOR_ID", "AUTHOR_NAME").count().show()
```

**输出:**

![Group by Clause](../Images/84c0a8aa932a1d96967863da22937da8.png)

<noscript><img class="alignnone size-full wp-image-568159" src="../Images/84c0a8aa932a1d96967863da22937da8.png" alt="Group by Clause" width="749" height="368" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Group-by-Clause.png 749w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Group-by-Clause-300x147.png 300w" sizes="(max-width: 749px) 100vw, 749px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Group-by-Clause.png"/></noscript>

pyspark github 中的 order by 函数用于按照我们使用的列对所有数据帧进行排序。它将按照列的值对数据帧进行排序。

**代码:**

```
func = spark.read.option("header", "true").csv("spark1.csv")
func.orderBy ("AUTHOR_ID").show()
```

**输出:**

![Sorting Data Frames](../Images/eb54288e60fbcdecb70b050dc252ba30.png)

<noscript><img class="alignnone size-full wp-image-568162" src="../Images/eb54288e60fbcdecb70b050dc252ba30.png" alt="Sorting Data Frames" width="751" height="352" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Sorting-Data-Frames.png 751w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Sorting-Data-Frames-300x141.png 300w" sizes="(max-width: 751px) 100vw, 751px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Sorting-Data-Frames.png"/></noscript>

### 例子

在下面的例子中，我们正在克隆 PySpark 简单项目。我们使用 git clone 命令克隆项目。克隆项目后，我们可以看到项目的所有文件都被复制到指定的目录中。

**代码:**

```
git clone */pyspark-examples.git
cd pyspark-examples.git
```

**输出:**

![PySpark GitHub - Project Files](../Images/32e20b51cc800385733e1405ea167c29.png)

<noscript><img class="alignnone wp-image-553462" src="../Images/32e20b51cc800385733e1405ea167c29.png" alt="PySpark GitHub - Project Files" width="643" height="391" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/09/KUYT.png 625w, https://cdn.educba.com/academy/wp-content/uploads/2022/09/KUYT-300x182.png 300w" sizes="(max-width: 643px) 100vw, 643px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/09/KUYT.png"/></noscript>

在克隆了下面例子中的项目后，我们通过对数据的操作来定义顺序。

**代码:**

```
import pyspark
from pyspark.sql import SparkSession
gith = SparkSession.builder.appName("pysparkdf").getOrCreate()
func = gith.read.option ("header", "true").csv("spark3.csv")
func.orderBy ("AUTHOR_ID").show()
```

**输出:**

![Order by Operation](../Images/8eed6feeb705bce8449f156770cb24bd.png)

<noscript><img class="alignnone size-full wp-image-568163" src="../Images/8eed6feeb705bce8449f156770cb24bd.png" alt="Order by Operation" width="751" height="346" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Order-by-Operation.png 751w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Order-by-Operation-300x138.png 300w" sizes="(max-width: 751px) 100vw, 751px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Order-by-Operation.png"/></noscript>

在下面的例子中，我们通过对数据的操作来定义分组。

**代码:**

```
import pyspark
from pyspark.sql import SparkSession
gith = SparkSession.builder.appName("pysparkdf").getOrCreate()
func = gith.read.option("header", "true").csv("spark3.csv")
func.groupBy ("AUTHOR_ID").count().show()
```

**输出:**

![Group by Operation](../Images/f04c239d172c6b8d32b328198364cb6e.png)

<noscript><img class="alignnone size-full wp-image-568167" src="../Images/f04c239d172c6b8d32b328198364cb6e.png" alt="Group by Operation" width="750" height="380" srcset="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Group-by-Operation.png 750w, https://cdn.educba.com/academy/wp-content/uploads/2022/08/Group-by-Operation-300x152.png 300w" sizes="(max-width: 750px) 100vw, 750px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2022/08/Group-by-Operation.png"/></noscript>

### 关键要点

*   GitHub 转换需要分区之间的数据洗牌和阶段边界的转换。调度程序将创建 PySpark GitHub 的执行盘。
*   基本上，GitHub 中有两种类型的转换可用。catalyst 正在定义 spark integral planner 的查询优化器。

### 常见问题解答

下面是提到的常见问题。

#### Q1。我们如何下载 PySpark GitHub 项目？

**答:**在使用 GitHub 项目时，我们可以从任何地方下载相同的项目，我们可以轻松地下载并运行相同的项目。

#### Q2。PySpark GitHub 中有多少种类型的转换？

**答:**py spark GitHub 中主要有两种类型的变换，宽变换和窄变换。宽变换在许多情况下被使用。

#### Q3。在运行 PySpark GitHub 项目时，我们需要使用哪个库？

**回答:**我们需要在运行 PySpark GitHub 项目的时候使用并导入 PySpark 库。

### 结论

在进行 PySpark 项目时，我们需要将 PySpark 的模块导入到我们的项目中。PySpark GitHub 是一个进程，我们通过它在指定的时间内使用 PySpark。为了创建 PySpark GitHub，用户需要使用 apache 提交 Spark 的应用程序。

### 推荐文章

这是 PySpark GitHub 的指南。在这里，我们讨论定义、什么是 PySpark GitHub、项目、功能、代码实现示例以及关键要点，以便更好地理解。您也可以看看以下文章，了解更多信息–

1.  [PySpark MLlib](https://www.educba.com/pyspark-mllib/)
2.  [PySpark 写 CSV](https://www.educba.com/pyspark-write-csv/)
3.  [PySpark Orderby](https://www.educba.com/pyspark-orderby/)
4.  [PySpark 将函数应用于列](https://www.educba.com/pyspark-apply-function-to-column/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>