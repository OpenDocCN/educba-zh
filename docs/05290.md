# 火花数据集

> 原文:[https://www.educba.com/spark-dataset/](https://www.educba.com/spark-dataset/)

![Spark Dataset](../Images/ed5ff3c55a26d84864d0de3d1a87124c.png)

<noscript><img class="alignnone size-full wp-image-225278" src="../Images/ed5ff3c55a26d84864d0de3d1a87124c.png" alt="Spark Dataset" width="900" height="500" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/Spark-Dataset.png"/></noscript>

## Spark 数据集简介

Spark 数据集是 SparkSQL 的基本数据结构之一。它有助于存储火花数据处理的中间数据。具有行类型的 Spark 数据集非常类似于在弹性分布式数据集(RDD)上以表格形式工作的数据框。Spark 中的数据集以其特定的特性而闻名，如类型安全、不变性、模式、性能优化、惰性评估、序列化和垃圾收集。数据集通过 Scala 和 Java 编程 API 得到支持。Spark 的数据集支持编译时安全和优化，这使它成为在 spark 框架中实现的首选。

### 为什么我们需要 Spark 数据集？

为了清楚地理解数据集，我们必须从火花和进化的历史开始。

<small>Hadoop、数据科学、统计学&其他</small>

RDD 是星火的核心。受 SQL 的启发，为了让事情变得更简单，Dataframe 是在 RDD 之上[创建的。Dataframe 相当于关系数据库中的表或 Python 中的 DataFrame。](https://www.educba.com/what-is-rdd/)

RDD 提供编译时类型安全，但是在 RDD 中没有自动优化。

Dataframe 提供了自动优化，但它缺乏编译时类型安全性。

数据集作为数据帧的扩展添加。数据集结合了 RDD 特性(即编译时类型安全)和数据框架(即 Spark SQL 自动优化)。

**【RDD(火花 1.0)】->【数据帧(火花 1.3)】->【数据集(火花 1.6)】**

因为数据集具有编译时安全性，所以它只在编译语言(Java & Scala)中受支持，而在解释语言(R & Python)中不受支持。[但是 Spark Dataframe](https://www.educba.com/spark-dataframe/) API 在 Spark 支持的所有四种语言(Java、Scala、Python & R)中都有。

| Spark 支持的语言。 | 数据帧 API | 数据集 API |
| 编译语言(Java 和 Scala) | 是 | 是 |
| 解释语言(R & Python) | 是 | 不 |

### 如何创建 Spark 数据集？

基于用例创建数据集有多种方式。

#### 1.首先创建 SparkSession

sparkSession 是 Spark 应用程序的单一入口点，它允许与底层 Spark 功能进行交互，并使用数据帧和数据集 API 对 Spark 进行编程。

`val spark = SparkSession
.builder()
.appName("SparkDatasetExample")
.enableHiveSupport()
.getOrCreate()`

*   使用范围、顺序、列表等基本数据结构创建数据集。:

**使用范围**
![spark dataset](../Images/e25d5ecd9bc4f74e310da88b97278bc5.png)

<noscript><img class="alignnone size-full wp-image-225203" src="../Images/e25d5ecd9bc4f74e310da88b97278bc5.png" alt="spark dataset" width="423" height="259" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset.png 423w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-300x184.png 300w" sizes="(max-width: 423px) 100vw, 423px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset.png"/></noscript>

**使用顺序**
![spark dataset 2](../Images/bf6ed0bad25e7a02a350a81c9dd61db8.png)

<noscript><img class="alignnone size-full wp-image-225204" src="../Images/bf6ed0bad25e7a02a350a81c9dd61db8.png" alt="spark dataset 2" width="519" height="218" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-2.png 519w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-2-300x126.png 300w" sizes="(max-width: 519px) 100vw, 519px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-2.png"/></noscript>

**使用列表**
![spark dataset 3](../Images/ba21cd3466f43aee5434daee7830a373.png)

<noscript><img class="alignnone wp-image-225205" src="../Images/ba21cd3466f43aee5434daee7830a373.png" alt="spark dataset 3" width="545" height="270" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-3.png 545w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-3-300x149.png 300w" sizes="(max-width: 545px) 100vw, 545px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-3.png"/></noscript>

*   方法，使用事例类序列创建数据集。toDS()方法:

![spark dataset 4](../Images/136c998cc925e703dac8fdb5fa9c9ab4.png)

<noscript><img class="alignnone size-full wp-image-225208" src="../Images/136c998cc925e703dac8fdb5fa9c9ab4.png" alt="spark dataset 4" width="644" height="256" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-4.png 644w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-4-300x119.png 300w" sizes="(max-width: 644px) 100vw, 644px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-4.png"/></noscript>

*   使用从 RDD 创建数据集。托德():

![spark dataset 5](../Images/eacc321ac034da313c62e96ff8771195.png)

<noscript><img class="alignnone size-full wp-image-225209" src="../Images/eacc321ac034da313c62e96ff8771195.png" alt="spark dataset 5" width="646" height="296" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-5.png 646w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-5-300x137.png 300w" sizes="(max-width: 646px) 100vw, 646px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-5.png"/></noscript>

*   要使用 Case 类从 Dataframe 创建数据集:

![Dataframe using Case Class](../Images/2311fd5a54343603e0072ea38a96daf5.png)

<noscript><img class="alignnone wp-image-225215 size-full" src="../Images/2311fd5a54343603e0072ea38a96daf5.png" alt="Dataframe using Case Class" width="643" height="298" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-6.png 643w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-6-300x139.png 300w" sizes="(max-width: 643px) 100vw, 643px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-6.png"/></noscript>

*   要使用元组从数据帧创建数据集:

![Dataframe using Tuples](../Images/dc3b79adbf783c0ed5e460a19035f76e.png)

<noscript><img class="alignnone wp-image-225216 size-full" src="../Images/dc3b79adbf783c0ed5e460a19035f76e.png" alt="Dataframe using Tuples" width="638" height="328" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-7.png 638w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-7-300x154.png 300w" sizes="(max-width: 638px) 100vw, 638px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-7.png"/></noscript>

#### 2.Spark 数据集上的操作

**1。字数示例**

![Word Count Example](../Images/18bfb5b669f5aeb92dac3cb1685e4677.png)

<noscript><img class="alignnone wp-image-225213 size-full" src="../Images/18bfb5b669f5aeb92dac3cb1685e4677.png" alt="Word Count Example" width="645" height="369" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-8.png 645w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-8-300x172.png 300w" sizes="(max-width: 645px) 100vw, 645px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-8.png"/></noscript>

**2。将火花数据集转换为数据帧**

我们还可以将 Spark 数据集转换为 Dataframe，并利用 Dataframe APIs，如下所示:

![dataframe](../Images/f0f6bfd98d501930a0af91577c22f80c.png)

<noscript><img class="alignnone wp-image-225218 size-full" src="../Images/f0f6bfd98d501930a0af91577c22f80c.png" alt="dataframe" width="638" height="347" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-9.png 638w, https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-9-300x163.png 300w" sizes="(max-width: 638px) 100vw, 638px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/10/spark-dataset-9.png"/></noscript>

### Spark 数据集的特征

下面是提到的不同特性:

**1。类型安全:** Dataset 提供编译时类型安全。这意味着应用程序的语法和分析错误将在运行前的编译时被检查。

**2。不变性:**数据集也像 RDD 和 Dataframe 一样是不可变的。这意味着我们不能改变创建的数据集。每次创建新数据集时，如果对数据集应用了任何转换。

**3。Schema:** Dataset 是一个内存中的表格结构，包含行和命名列。

**4。性能和优化:**与 Dataframe 一样，数据集也使用 Catalyst 优化来生成优化的逻辑和物理查询计划。**T3】**

**5。编程语言:**dataset API 只存在于 Java 和 Scala 这两种编译语言中，而不存在于 Python 这种解释型语言中。

**6。惰性求值:**像 RDD 和 Dataframe 一样，数据集也执行惰性求值。这意味着只有当动作被执行时，计算才会发生。Spark 只在转型阶段制定计划。

**7。序列化和垃圾收集:spark** 数据集不使用标准序列化器(Kryo 或 Java 序列化)。相反，它使用钨的快速内存编码器，了解数据的内部结构，可以有效地将对象转换为内部二进制存储。它使用钨编码器进行堆外数据序列化，因此不需要垃圾收集。

### 结论

数据集是 RDD 和数据框架的最佳选择。RDD 提供了编译时类型安全，但是没有自动优化。Dataframe 提供了自动优化，但它缺乏编译时类型安全性。数据集提供编译时类型安全和自动优化。因此，数据集是使用 Java 或 Scala 的 Spark 开发人员的最佳选择。

### 推荐文章

这是 Spark 数据集的指南。在这里，我们将讨论如何以多种方式通过示例和功能创建 Spark 数据集。您也可以看看以下文章，了解更多信息–

1.  [火花外壳命令](https://www.educba.com/spark-shell-commands/)
2.  [星火生涯](https://www.educba.com/career-in-spark/)
3.  [火花流](https://www.educba.com/spark-streaming/)
4.  [Spark SQL 数据帧](https://www.educba.com/spark-sql-dataframe/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>