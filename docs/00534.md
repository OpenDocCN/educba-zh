# 正则化机器学习

> 原文:[https://www.educba.com/regularization-machine-learning/](https://www.educba.com/regularization-machine-learning/)

![Regularization techniques](../Images/23ba15a51aca38e9d15d37d540fb15e0.png)

<noscript><img class="alignnone size-full wp-image-291049" src="../Images/23ba15a51aca38e9d15d37d540fb15e0.png" alt="Regularization techniques" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-Machine-Learning.jpg 824w, https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-Machine-Learning-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-Machine-Learning-768x428.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-Machine-Learning.jpg"/></noscript>

## 正则化机器学习简介

以下文章提供了正则化机器学习的概述。正则化是添加数据以解决不适定缺点或防止过度拟合的方法。它适用于不适定改进问题中的目标函数。通常，回归模型会过度拟合它所指导的信息。在正则化方法中，我们倾向于尝试减少回归运算的复杂性，而不是真正降低基础多项式运算的次数。正则化通常是作为一种方法来提高学习模型的概化能力。

**关于正则化机器学习的更多信息:**

<small>Hadoop、数据科学、统计学&其他</small>

*   正则化甚至是为了分类。作为分类器通常是一个不确定的缺点，因为它试图推断操作的任何 x 给定。
*   术语正则化是对损失运算补充。
*   正规化将发挥多种功能，同时学习更容易分发的模型，并将集群结构引入教育缺陷。
*   这个学习缺点的目标是寻找匹配或预测结果的操作，该结果最小化总体潜在输入和标签的预期误差。

### 吉洪诺夫正则化

吉洪诺夫正则化经常以后续方式使用。

向前推一个非正则损失函数 l0(例如总平方误差)和模型参数 w，正则损失操作变成:

![Regularization Machine Learning 1](../Images/0a102bca5c4b2f07e15796af2eb6c0da.png)

<noscript><img class="alignnone wp-image-290678 size-full" src="../Images/0a102bca5c4b2f07e15796af2eb6c0da.png" alt="Regularization Machine Learning 1" width="434" height="47" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Tikhonov-regularization.png 434w, https://cdn.educba.com/academy/wp-content/uploads/2020/01/Tikhonov-regularization-300x32.png 300w" sizes="(max-width: 434px) 100vw, 434px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Tikhonov-regularization.png"/></noscript>

在 L2 正则化的情况下，L 采用标量乘以单位矩阵或权重的平方和的形状。

一些常用的正则化技术包括:

![Regularization techniques](../Images/762353cdb41df41b73dc3c274c4de647.png)

<noscript><img class="alignnone wp-image-291045" src="../Images/762353cdb41df41b73dc3c274c4de647.png" alt="Regularization techniques" width="475" height="240" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-techniques.jpg 508w, https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-techniques-300x152.jpg 300w" sizes="(max-width: 475px) 100vw, 475px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Regularization-techniques.jpg"/></noscript>

*   L1 正则化
*   L2 正则化
*   提前停止
*   辍学正规化
*   培训知识扩充
*   批量标准化

#### 1.拉索正则化(L1 正则化)

正则化或套索正则化增加了对错误操作的惩罚。惩罚是绝对重量值的总和。

![Regularization Machine Learning 2](../Images/b332c4c3e31653783e6fcc67528860b6.png)

<noscript><img class="alignnone wp-image-290680 size-full" src="../Images/b332c4c3e31653783e6fcc67528860b6.png" alt="Regularization Machine Learning 2" width="355" height="87" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Lasso-regularization.png 355w, https://cdn.educba.com/academy/wp-content/uploads/2020/01/Lasso-regularization-300x74.png 300w" sizes="(max-width: 355px) 100vw, 355px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Lasso-regularization.png"/></noscript>

p 是标准化参数，决定我们希望惩罚模型的比例。

这种套索正则化也被称为 L1 正则化。

#### 2.岭正则化(L2 正则化)

L2 正则化或岭正则化联合地把一个惩罚加到误差操作上。然而，这里的惩罚是权重平方值的总和。

![Ridge regularization](../Images/8e5052f283534caefac7cc8d72838a9e.png)

<noscript><img class="alignnone size-full wp-image-290681" src="../Images/8e5052f283534caefac7cc8d72838a9e.png" alt="Ridge regularization" width="328" height="74" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Ridge-regularization.png 328w, https://cdn.educba.com/academy/wp-content/uploads/2020/01/Ridge-regularization-300x68.png 300w" sizes="(max-width: 328px) 100vw, 328px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/01/Ridge-regularization.png"/></noscript>

p 是标准化参数，决定我们希望惩罚模型的比例。

这种脊正则化也被称为 L2 正则化。

这两种技术的区别在于，lasso 将较小的选项常数缩小到零，从而完全删除了一些功能。因此，如果我们有大量的选项，这对于功能选择很有用。

#### 3.早期停止正则化

早停是思想习惯了垄断过度。在这种情况下，信息集被用来在每个训练时期的顶部计算损失，并且一旦损失停止减少，就停止训练并使用检查知识来计算最终的分类精度。早期停止通常单独使用或与正则化技术结合使用。

最好的结局往往被认为是因为超参数，因此我们倾向于在一次教练运行的整个过程中测试超参数的多个值。这使得早期停止比不同的参数改进技术更经济，这些技术通常需要模型的完整运行来检查一个炒作参数的价值。早期停止可能是一种相当不引人注目的正则化，因为它不需要对模型或目标进行任何可能改变系统教育动态的修改。

#### 4.辍学正规化

辍学是几年前出现的最有效的正规化技术之一。辍学背后的基本计划是在第一个 DLN 的随意改变版本上运行场景公式的每一次迭代。

*   辍学迫使神经网络被告知许多可靠的选项，这些选项与相反神经元的许多可选随机子集相结合是有帮助的。
*   放弃大约会使收敛所需的迭代次数加倍。然而，每个时期的指导时间是一个较小的量。
*   有了 h 个隐藏单元，每一个都可能诞生，我们就有了
    2^H 潜在模型。在测试部分，考虑了完整的网络，并且每次激活都减少了一个元素 p。

#### 5.培训知识扩充

如果用于指导的信息集不够大，这通常是几个真实世界检查集的情况，那么它将导致过度拟合。避免这一缺点的一种直接技术是通过人工手段增加教练集。

我们将能够使图片经受后续的变换，而不动态地对其进行分类:

*   翻译
*   旋转
*   反射
*   偏离
*   缩放比例
*   改变区别或亮度

所有这些变化都是人眼用来体验的。但有不同的增强技术不属于这一类别，比如向教练知识集添加随机噪声，只要它是严格完成的，就会非常有效。

#### 6.批量标准化

输入层的数据标准化可以是一种返工信息的方式，以加快改进方法。既然标准化是有用的，为什么不把它扩展到网络内部，使所有的激活标准化呢？这可能正是被称为批量标准化的公式中被删除的内容。

对计算机文件使用标准化操作是一个简单的练习，因为完整的教练知识集在教练方法的开始是可访问的。隐藏层激活不会出现这种情况，因为这些值会因为系统参数的公式驱动更新而在指导过程中修改。Ioffe 和 Szegady 通过分批进行标准化(因此得名)解决了这一缺点，这样在每一批中参数都保持固定。

### 结论——正则化机器学习

正则化引入了用于探索通常建立模型的操作区域的边界区域的惩罚，这可以提高泛化能力。一旦模型学习了教练知识中的细节和噪声，过度拟合可能是一种发展，这种发展会对模型在新知识上的性能产生负面影响。

### 推荐文章

这是一个正则化机器学习的指南。这里我们讨论正则化机器学习的介绍以及不同类型的正则化技术。您也可以看看以下文章，了解更多信息–

1.  [机器学习数据集](https://www.educba.com/machine-learning-datasets/)
2.  [监督机器学习](https://www.educba.com/supervised-machine-learning/)
3.  [机器学习生命周期](https://www.educba.com/machine-learning-life-cycle/)
4.  [机器学习中的朴素贝叶斯](https://www.educba.com/naive-bayes-in-machine-learning/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>