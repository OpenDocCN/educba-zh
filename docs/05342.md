# PySpark 过滤器

> 原文:[https://www.educba.com/pyspark-filter/](https://www.educba.com/pyspark-filter/)

![PySpark Filter](../Images/2d57827e684546165647f5a8076848e1.png)

<noscript><img class="alignnone size-full wp-image-456683" src="../Images/2d57827e684546165647f5a8076848e1.png" alt="PySpark Filter" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter.jpg"/></noscript>

## PySpark 滤波器简介

PySpark Filter 是 PySpark 中添加的一个函数，用于处理 Spark 数据帧中需要过滤的数据。在 PySpark 和 PYSPARK Filter 中处理数据时，数据清理是一项非常重要的任务。PySpark 过滤器与数据帧一起使用，用于始终过滤数据，以便留下需要的数据进行处理，而不使用剩余的数据。这有助于更快地处理数据，因为在数据帧中使用过滤操作清除了不需要的或坏的数据。

PySpark 过滤条件是应用在数据帧上的，有几个基于数据过滤数据的条件，这些条件可以是单个条件，也可以是使用 SQL 函数的多个条件。从 RDD /数据框中过滤出这些行，并将结果用于进一步处理。

<small>网页开发、编程语言、软件测试&其他</small>

**语法:**

PySpark 滤波器函数的语法是:

`df.filter(#condition)`

*   **df:**py spark 数据帧
*   **条件:**我们要实现的过滤条件。

**截图:**

![PySpark Filter 1](../Images/b44249cf978b4c39d7402d99bbe11a21.png)

<noscript><img class="alignnone size-full wp-image-456466" src="../Images/b44249cf978b4c39d7402d99bbe11a21.png" alt="PySpark Filter 1" width="246" height="29" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-1.png"/></noscript>

### PySpark 中滤波器的工作

让我们看看 PySpark 中的过滤函数是如何工作的

过滤函数根据条件从数据帧中提取数据。

首先计算函数中定义的条件，然后返回包含满足条件的数据的行，不满足条件的行返回。filter 函数首先检查一个条件下的所有行，方法是检查列和内部写入的条件，并根据需要的结果对每一行进行评估。

过滤条件类似于 SQL 中的 where 条件，它根据提供的条件过滤数据。

### 例子

让我们看一些 PySpark 滤波器函数如何工作的例子:

让我们首先创建一个简单的数据框，我们希望使用过滤操作。

**数据帧的创建:**

`a= spark.createDataFrame(["SAM","JOHN","AND","ROBIN","ANAND"], "string").toDF("Name")`

让我们从一个简单的过滤代码开始，它过滤数据框中的名称。

`a.filter(a.Name == "SAM").show()`

这被应用于 Spark 数据帧并过滤其中具有 SAM 名称的数据。

输出将返回一个包含满意数据的数据帧。

相同的数据可以被过滤掉，我们可以根据处理的需要在数据上加上条件。

`a.filter(a.Name == "JOHN").show()`

这将打印带有过滤条件的名为 JOHN 的数据帧。

这里我们使用数据帧的方法。列方法作为过滤和获取数据的方法。

另一种获取列数据的方法是在 PySpark SQL 中使用简单的 SQL column 方法。

这可以通过导入 SQL 函数并在其中使用 col 函数来实现。

`from pyspark.sql.functions import col
a.filter(col("Name") == "JOHN").show()`

这将过滤数据帧，并产生与上述示例相同的结果。

约翰被过滤，结果显示回来。

**输出:**

**截图:**

![PySpark Filter 2](../Images/adc1283af2d155d96cefdca0a6ac9fc4.png)

<noscript><img class="alignnone size-full wp-image-456467" src="../Images/adc1283af2d155d96cefdca0a6ac9fc4.png" alt="PySpark Filter 2" width="340" height="103" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-2.png 340w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-2-300x91.png 300w" sizes="(max-width: 340px) 100vw, 340px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-2.png"/></noscript>

如果我们尝试使用 SQL 方法，同样可以做到这一点。只需要在过滤条件中传递条件，就像用 SQL 语句传递一样。

`a.filter("Name == 'JOHN'")`

通过这种方式，我们可以直接将一个语句作为数据帧的条件语句，并产生相同的输出。

`a.filter("Name == 'JOHN'").show()`

**截图:**

![Output 3](../Images/406c235f061e1e41fbb66b420fa94071.png)

<noscript><img class="alignnone wp-image-456469 size-full" src="../Images/406c235f061e1e41fbb66b420fa94071.png" alt="Output 3" width="625" height="166" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-3.png 625w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-3-300x80.png 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-3-620x166.png 620w" sizes="(max-width: 625px) 100vw, 625px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-3.png"/></noscript>

我们还可以使用多个操作符来过滤 PySpark DataFrame 的数据，并相应地过滤数据。

也可以使用 AND 和 OR 运算符来过滤数据。

让我们通过创建一个包含多列的数据框来验证这一点。

`data1= [{'Name':'Jhon','ID':2,'Add':'USA'},{'Name':'Joe','ID':3,'Add':'MX'},{'Name':'Tina','ID':4,'Add':'IND'}] rd1 = sc.parallelize(data1)
df1 = spark.createDataFrame(rd1)`

这将创建一个名为 DF1 的 DataFrame，其中有多个列作为 Name、Add 和 ID。

现在让我们来看看对多列的过滤操作的使用。

`df1.filter("Add = 'USA' or Name = 'Jhon'").show()`

这将过滤名称为 Jhon 并添加为 USA 的所有列。

**截图:**

![Output 4](../Images/f488544b52239119cb5d15a816b0eb46.png)

<noscript><img class="alignnone wp-image-456470 size-full" src="../Images/f488544b52239119cb5d15a816b0eb46.png" alt="Output 4" width="431" height="97" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-4.png 431w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-4-300x68.png 300w" sizes="(max-width: 431px) 100vw, 431px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-4.png"/></noscript>

AND 运算符也适用于这种情况。这将仅在两个条件都为真时过滤数据。

`df1.filter("Add = 'USA' and Name = 'Jhon'").show()`

**截图:**

![PySpark Filter 5](../Images/f33d3cdf4d5f56acc8bfbfec6258e946.png)

<noscript><img class="alignnone size-full wp-image-456471" src="../Images/f33d3cdf4d5f56acc8bfbfec6258e946.png" alt="PySpark Filter 5" width="452" height="98" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-5.png 452w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-5-300x65.png 300w" sizes="(max-width: 452px) 100vw, 452px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/PySpark-Filter-5.png"/></noscript>

如果任何结果为负，则返回空数据帧。

`df1.filter("Add = 'USA' and Name = 'Jhn'").show()`

**截图:**

![example 1](../Images/dd9ff41bb62cd09d6a7d8850fe5b4e69.png)

<noscript><img class="alignnone size-full wp-image-456472" src="../Images/dd9ff41bb62cd09d6a7d8850fe5b4e69.png" alt="example 1" width="436" height="89" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/02/example-1-6.png 436w, https://cdn.educba.com/academy/wp-content/uploads/2021/02/example-1-6-300x61.png 300w" sizes="(max-width: 436px) 100vw, 436px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/02/example-1-6.png"/></noscript>

这是 PySpark 中过滤函数的一些例子。

### 结论

从上面的文章中，我们看到了 PySpark 中过滤器操作的使用。通过各种示例和分类，我们试图理解 PySpark 中的 FILTER 方法是如何工作的，以及在编程级别使用了什么。

我们还看到了 Spark 数据框架中过滤器的内部工作和优势，以及它在各种编程目的中的使用。此外，语法和例子帮助我们更准确地理解函数。

### 推荐文章

这是 PySpark 滤波器的指南。这里我们讨论 PySpark 中 Filter 的介绍、语法、工作原理，以及代码实现的例子。您也可以看看以下文章，了解更多信息–

1.  [PySpark 加入](https://www.educba.com/pyspark-join/)
2.  [卡尔曼滤波器 Matlab](https://www.educba.com/kalman-filter-matlab/)
3.  [PySpark SQL](https://www.educba.com/pyspark-sql/)
4.  [火花平面图](https://www.educba.com/spark-flatmap/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>