# Spark Shell 命令

> 原文：<https://www.educba.com/spark-shell-commands/>

![Spark Shell Commands](img/6a42132d8bc2b59f52a4af57aec71870.png)



## 什么是 Spark Shell 命令？

Spark Shell 命令是用于操作 Spark 处理的命令行界面。Spark Shell 命令对于通过在大容量数据集上以非常少的时间实现机器学习来处理 ETL 和分析非常有用。spark 中使用的 shell 命令主要有三种，比如 scala 的 spark-shell、python 的 pyspark 和 R 语言的 spark。Spark-shell 使用 scala 和 java 语言作为环境的先决条件。有特定的 spark shell 命令可用于执行 Spark 操作，例如检查 Spark 的安装版本，创建和管理称为 RDD 的弹性分布式数据集。

### Spark Shell 命令的类型

各种 Spark-shell 命令如下:

<small>Hadoop、数据科学、统计学&其他</small>

1.为了检查是否安装了 Spark 并了解其版本，使用了以下命令(此后的所有命令均以符号“$”开头)

**$火花壳**

如果安装了 spark，将显示以下输出:

**$火花壳**

使用 Spark2 将 SPARK_MAJOR_VERSION 设置为 2

将默认日志级别设置为“警告”。

若要调整日志记录级别，请使用 sc.setLogLevel(newLevel)。对于 SparkR，使用 setLogLevel(newLevel)。

Spark context Web UI 可从 http://10.113.59.34:4040 获得

Spark 上下文可用作“sc”(master = local[*]，app id = local-1568732886588)。

Spark 会话可作为“spark”使用。

欢迎来到

____ __

/ __/__ ___ _____/ /__

_\ \/ _ \/ _ `/ __/ '_/

/___/ .__/\_，_/_/ /_/\_\版本 2.2.0.2.6.3.0-235

/_/

使用 Scala 版本 2.11.8 (Java HotSpot(TM) 64 位服务器虚拟机，Java 1.8.0_112)

键入表达式进行评估。

键入:help 了解更多信息。

scala >

2.Spark 的基本数据结构称为 RDD(弹性分布式数据集),它包含一个不可变的对象集合，用于记录的分布式计算。RDD 的所有数据集都跨集群的多个节点进行逻辑分区。

只能通过从本地文件系统读取或转换现有 RDD 来创建 RDD。

a)要创建新的 RDD，我们使用以下命令:

`scala> val examplefile = sc.textFile("file.txt")`

这里 sc 被称为 SparkContext 的对象。

**输出:**

`examplefile: org.apache.spark.rdd.RDD[String] = file.txt MapPartitionsRDD[3] at textFile at <console>:24`

b)可以通过并行收集创建 RDD，如下所示:

`scala> val oddnum = Array(1, 3, 5, 7, 9)`

**输出:**

`oddnum: Array[Int] = Array(1, 3, 5, 7, 9)
scala> val value = sc.parallelize(oddnum)`

**输出:**

`value: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:26`

c)从现有的 RDD**:**中创建

`scala> val newRDD = oddnum.map(value => (value * 2))`

**输出:**

`newRDD: Array[Int] = Array(2, 6, 10, 14, 18)`

3.有两种类型的火花 RDD 操作可在创建的数据集上执行:

*   行动
*   转换

**动作:**用于对现有数据集执行某些必需的操作。以下是一些可用于对创建的数据集执行以下操作的命令:

a) count()函数计算 RDD 中元素的数量:

`scala> value.count()`

**输出:**

`res3: Long = 5`

b) collect()函数显示数组的所有元素:

`scala> value.collect()`

**输出:**

`res5: Array[Int] = Array(1, 3, 5, 7, 9)`

c) first()函数用于显示数据集的第一个元素:

`scala> value.first()`

**输出:**

`res4: Int = 1`

d) take(n)函数显示数组的前 n 个元素:

`scala> value.take(3)`

**输出:**

`res6: Array[Int] = Array(1, 3, 5)`

e) takeSample (withReplacement，num，[seed])函数显示一个由“num”个元素组成的随机数组，其中的种子是用于[随机数生成器](https://www.educba.com/random-number-generator-in-matlab/)的。

`scala> value.takeSample(false, 3, System.nanoTime.toInt)`

**输出:**

`res8: Array[Int] = Array(3, 1, 7)`

f) saveAsTextFile(path)函数将数据集保存在 hdfs 位置的指定路径中

`scala> value.saveAsTextFile("/user/valuedir")`

g)隔板。长度函数可用于找出 RDD 中的分区数

`scala> value.partitions.length`

**输出:**

`res1: Int = 8`

### RDD 变换

转换是用来从现有的 RDD 形成一个新的。由于 RDD 的输入是不可变的，转换后形成的结果可以是一个或多个 RDD 作为输出。

有两种类型的转换:

*   狭义变换
*   广泛的变革

**窄转换–**每个父 RDD 被划分成不同的分区，其中只有一个分区将被子 RDD 使用。

**举例:** map()和 filter()是两种基本类型的基本转换，在调用一个动作时调用。

*   map(func)函数对数据集“值”中的每个元素进行迭代运算，以生成输出 RDD。

**示例:**在这个示例中，我们将值 10 添加到数据集值的每个元素中，并在 collect 函数的帮助下显示转换后的输出。

`scala> val mapfunc = value.map(x => x+10)
mapfunc: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at <console>:28`

`scala> mapfunc.collect
res2: Array[Int] = Array(11, 13, 15, 17, 19)`

filter(func)函数主要用于过滤掉满足使用函数指定的特定条件的元素。

**示例:**在这个示例中，我们试图检索除数据集“value”的编号 2 之外的所有元素，并通过 collect 函数获取输出。

`scala> val fill = value.filter(x => x!=2)
fill: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at filter at <console>:28`

`scala> fill.collect
res8: Array[Int] = Array(4, 6, 8, 10)`

**大范围转换**–一个父 RDD 分区被它的多个子 RDD 分区共享。

**示例:** groupbykey 和 reducebyKey 就是宽变换的例子。

*   groupbyKey 函数根据另一个 RDD 的键值将数据集值分组为键值对。当 group by 函数收集与特定键相关联的数据并将它们存储在单个键-值对中时，这个过程涉及到洗牌。

**示例:**在这个示例中，我们将整数 5，6 分配给字符串值“key ”,将整数 8 分配给“8 ”,它们在输出中以相同的键-值对格式显示。

`scala> val data = spark.sparkContext.parallelize(Array(("key",5),("val",8),("key",6)),3)
data: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[13] at parallelize at <console>:23`

`scala> val group = data.groupByKey().collect()
group: Array[(String, Iterable[Int])] = Array((key,CompactBuffer(5, 6)), (val,CompactBuffer(8)))`

`scala> group.foreach(println)
(key,CompactBuffer(5, 6))
(val,CompactBuffer(8))`

*   reduceByKey 函数还组合来自不同 RDD 的键值对。在执行上述转换后，它将键和它们各自的值组合成一个元素。

**示例:**在这个示例中，数组“letters”的公共键首先被函数并行化，每个字母被映射到 count 10。reduceByKey 将添加具有相似关键字的值，并保存在变量 value2 中。然后使用 collect 函数显示输出。

`scala> val letters = Array("A","B","C","D","B","C","E","D")
letters: Array[String] = Array(A, B, C, D, B, C, E, D)`

`scala> val value2 = spark.sparkContext.parallelize(letters).map(w => (w,10)).reduceByKey(_+_)
value2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[20] at reduceByKey at <console>:25`

`scala> value2.foreach(println)
(C,20)
(E,10)
(D,20)
(B,20)
(A,10)`

除了上面提到的对 RDD 进行分区和执行操作/转换之类的操作，Spark 还支持缓存，这在递归调用相同数据时很有帮助。

借助所有这些属性，Apache Spark 可以处理海量数据，并执行批处理和流处理。Spark 完成的内存计算负责应用程序的极快处理。因此，Spark 是首选方法，因为它具有跨不同语言编程的多功能性、易用性和集成能力。

### 推荐文章

这是 Spark Shell 命令的指南。这里我们讨论不同编程语言的各种类型的 Spark Shell 命令。您也可以阅读以下文章，了解更多信息——

1.  [外壳脚本命令](https://www.educba.com/shell-scripting-commands/)
2.  [如何安装 Spark](https://www.educba.com/how-to-install-spark/)
3.  [星火面试试题](https://www.educba.com/spark-interview-questions/)
4.  [火花命令](https://www.educba.com/spark-commands/)
5.  [临时测试](https://www.educba.com/adhoc-testing/)
6.  [JavaScript 中的随机数生成器](https://www.educba.com/random-number-generator-in-javascript/)
7.  [Unix Shell 命令列表指南](https://www.educba.com/unix-shell-commands/)
8.  [PySpark SQL | PySpark SQL 的模块和方法](https://www.educba.com/pyspark-sql/)
9.  [Shell 脚本中的 For 循环| For 循环是如何工作的？](https://www.educba.com/for-loop-in-shell-scripting/)
10.  [带示例的批处理脚本命令](https://www.educba.com/batch-scripting-commands/)
11.  [火花部件的完整概述](https://www.educba.com/spark-components/)





