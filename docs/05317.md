# PySpark 备忘单

> 原文：<https://www.educba.com/pyspark-cheat-sheet/>

![PySpark Cheat Sheet](img/7debea9a1dd602d217292b23cdbf341f.png)



## PySpark 备忘单介绍

Pyspark cheat sheet 是 apache 的 API，我们可以使用 python 来处理 RDS。Apache spark 是一种快速的开源引擎，通过内置的 SQL 和机器学习模块来处理大数据，也用于图形处理。与其他技术相比，它使我们能够将分析应用程序的速度提高 100 倍。

### PySpark 备忘单的定义

PySpark cheat sheet 在数据工程中很重要，随着对数据处理需求的增加，企业将整合由 apache 维护的用于处理大量数据的数据栈。备忘单中最重要的是数据块。我们可以说小抄比熊猫小抄更快。它包含初始化和加载我们的数据以检索 RDD 的信息，该信息对我们的数据进行排序、过滤和采样。它还用于在 python PySpark 中创建信息丰富且有吸引力的表单。

<small>Hadoop、数据科学、统计学&其他</small>

### PySpark 备忘单配置

我们可以如下配置备忘单。对于配置，我们需要遵循以下步骤。在第一个例子中，我们使用 pip 命令安装 PySpark。

1.第一步，我们使用 pip 命令安装 PySpark 模块，如下所示。我们也可以使用另一个命令来安装。

```
pip install pyspark
```

![Pyspark cheat sheet Configuration 1](img/1e60f2c6f840a0113c5498367cbe5797.png)



2.要检查 PySpark 模型是否安装在我们的系统中，我们需要登录 python 模块。以下示例显示了登录 python shell。

```
python
```

![Pyspark cheat sheet Configuration 2](img/e18dc13ffbfb79dfe1eac05827491bdc.png)



3.当在这一步登录到 python 的 shell 中时，我们正在导入 PySpark 模块。我们使用 import 关键字导入模块，如下所示。在下面的例子中，我们如下导入 spark 配置和 spark 上下文模块。

```
from pyspark import SparkConf, SparkContext
```

![Pyspark cheat sheet Configuration 3](img/a8c015afee87b6eff6e8f48b176ec0d0.png)



4.在本步骤中导入模块后，我们将如下配置备忘单模块。我们将应用程序命名为 PySpark config。

```
pyspark_conf = (SparkConf()
.setMaster ("local")
.setAppName ("Pyspark config")
. set ("spark. executor.memory",   "lg"))
```

![Pyspark cheat sheet Configuration 4](img/69317eb184361c9975d379940c0a7330.png)



5.在定义了备忘单的配置之后，现在我们正在配置备忘单。在下面的例子中，我们将变量名定义为 pyspark_cheat。

```
pyspark_cheat = SparkContext (conf = pyspark_conf)
```

![Pyspark cheat sheet Configuration 5](img/21180a47ce4978fb7c7e390a7472c8db.png)



### 初始化

在使用备忘单时，初始化是一个非常重要的步骤，没有初始化我们就无法创建备忘单。下面的步骤显示了我们如何如下初始化它。

a)第一步，我们导入 PySpark 模块。我们使用 import 关键字导入 spark 上下文模块，如下所示。

```
from pyspark import SparkContext
```

![Initialization 1](img/1dc728a81d02b41de4f138fd49892ba1.png)



b)在本步骤中导入模块后，我们将创建如下 spark 上下文。我们将变量名定义为 config。

```
config = SparkContext(master = 'local[2]')
```

![Initialization 2](img/97ea159702902040817f79163bbff91a.png)



c)在本步骤中创建 spark 上下文后，我们将检查 spark 上下文和 python 的版本，如下所示。在下面的例子中，spark 上下文版本是 3.3.0，python 版本是 3.10。

```
config.version
config.pythonVer
```

![Initialization 3](img/c5fb6dbb893f7b6b734a6b88d8fb7165.png)



d)在定义了版本之后，在这一步中，我们将连接到主 URL，我们还将检查 spark 的安装路径。

```
config.master
str(config.sparkHome)
```

![Initialization 4](img/b587e5fe34bc9648ccfe2b526064053d.png)



e)连接到主 URL 后，现在我们正在检索主用户的名称。我们可以看到主用户的名字是 OMSAI。

```
str(config.sparkUser())
```

![Initialization 5](img/1d171e87e93e1631a6d3d74b9bd662b6.png)



f)在下面的示例中，我们检查初始化的应用程序的应用程序名称和应用程序 id，如下所示。在下面的例子中，我们可以看到应用程序名 pyspark-shell。

```
config.applicationId
config.appName
```

![Initialization 6](img/d41b6d184967840bbf0101baaaa6c1a0.png)



g)在检查应用程序名称和应用程序 ID 之后，在这一步中，我们将返回默认的并行度和分区级别，如下所示。

```
config.defaultParallelism
config.defaultMinPartitions
```

![Initialization 7](img/bd6f60900e163ac6b238bdf65bbad137.png)



### 如何创建 PySpark 备忘单数据框架？

以下步骤显示了我们如何创建数据框。

1.在第一步中，我们使用 import 命令导入 PySpark sql 模块，如下所示。

```
from pyspark.sql.types import*
```

![Create Dataframe 1](img/a77e2650ef0251339329ed9d7e2416f9.png)



2.在本步骤中导入模块后，我们将配置 spark 上下文，并加载数据帧的数据，如下所示。

```
py = sc.parallelize ([('p', 5), ('q', 3), ('r',3)])
```

![Create Dataframe 2](img/a945bbe455fef9ed0e356914bf4fc41a.png)



3.在下面的示例中，我们还加载了用于创建数据框的数据，如下所示。

```
pyspark1 = sc.parallelize ([('p', 3), ('s', 2), ('r', 2)])
pyspark2 = sc.parallelize(range (100))
```

![Create Dataframe 3](img/46cb7f3e0561142d44c786c8a74f61d4.png)



4.在本步骤中加载数据后，我们将使用上述数据创建数据框，如下所示。

```
py = sc.parallelize ([("p",["a", "b", "c"]),
("q" ["x", "z"])])
```

![Create Dataframe 4](img/f317c2c4d6de27ff568b34e62e4f41dd.png)



### 例子

下面是一些例子。

#### 示例#1

这里我们正在配置备忘单。

**代码:**

```
from pyspark import SparkConf, SparkContext
conf = (SparkConf()
.setMaster ("py_local")
.setAppName("Pyspark local config")
. set ("spark.executor.memory", "lg"))
pyspark_cheat = SparkContext (conf = conf)
```

**输出:**

![PySpark Cheat Sheet Example 1](img/3321eebe8dd141dbcfd1004a128132f6.png)



#### 实施例 2

在下面的例子中，我们正在加载数据。

**代码:**

```
from pyspark.sql import Row
py1 = Row (original_title = 'ABC', budget = '1', year = 2001)
py2 = Row (original_title = 'PQR', budget = '2', year = 2002)
pyspark = [py1, py2]
cheat = spark.createDataFrame (pyspark)
cheat.show ()
```

**输出:**

![PySpark Cheat Sheet Example 2](img/b46e1a198a63bf44c664f4b82db8b5ef.png)



### 关键要点

*   在处理 PySpark 备忘单时，我们需要首先初始化 PySpark 备忘单，如果不进行初始化，我们就无法创建备忘单。
*   在使用这个备忘单时，我们需要使用 import 关键字导入 PySpark 模块。

### 常见问题解答

下面是提到的常见问题:

#### Q1。python 中的 PySpark 备忘单有什么用？

**回答:**我们正在使用一个 python 备忘单来更清晰、更非正式地读取数据。python 备忘单在 python 中很重要。

#### Q2。PySpark 备忘单中的数据框有什么用？

**回答:**数据框对于创建相同的数据框很有用。我们可以使用文本文件或 CSV 文件来创建 PySpark 备忘单。

#### Q3。我们如何修改 PySpark 备忘单中的数据框？

**答:**数据帧将从 RDD 抽象出来。数据集不是表格形式的，我们通过文本或 CSV 文件加载数据集。

### 结论

PySpark 备忘单中最重要的是数据块。PySpark 备忘单在数据工程中很重要。Apache spark 被称为快速和开源引擎，用于处理 SQL 和机器学习的内置模块的大数据，也用于图形处理。

### 推荐文章

这是 PySpark 备忘单指南。在这里，我们讨论了简介、配置、初始化以及如何创建 PySpark 备忘单数据框，并给出了示例。您也可以看看以下文章，了解更多信息–

1.  [PySpark 写 CSV](https://www.educba.com/pyspark-write-csv/)
2.  [PySpark MLlib](https://www.educba.com/pyspark-mllib/)
3.  [PySpark 数据帧](https://www.educba.com/pyspark-dataframe/)
4.  PySpark fillna





