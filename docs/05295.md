# 火花并行化

> 原文:[https://www.educba.com/spark-parallelize/](https://www.educba.com/spark-parallelize/)

![Spark Parallelize](../Images/9fb5a513f778b21df5020065267196ed.png)

<noscript><img class="alignnone size-full wp-image-363017" src="../Images/9fb5a513f778b21df5020065267196ed.png" alt="Spark Parallelize" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize.jpg"/></noscript>

## Spark 并行化简介

并行化是一种从驱动程序中的现有集合(例如数组)创建 RDD 的方法。集合中存在的元素被复制以形成分布式数据集，我们可以在其上并行操作。在本主题中，我们将了解 Spark 并行化。

并行化是在 spark 中创建 RDD 的三种方法之一，其他两种方法是:

<small>Hadoop、数据科学、统计学&其他</small>

*   从外部数据源，如本地文件系统、HDFS、卡珊德拉等。
*   通过在现有 RDD 上运行转换操作。

**语法:**

`sc.parallelize (seq: Seq[T],numSlices: Int)`

这里 sc 是 SparkContext 对象，

seq 是存在于驱动程序中的集合对象，

numSlices 是一个可选参数，它表示将为数据集创建的分区数量。Spark 为每个分区运行一个任务。因此，它在控制被执行的并行操作数量的意义上是重要的。

### 方法怎么用？

为了使用 parallelize()方法，首先要创建一个 SparkContext 对象。

它可以通过以下方式创建:

**1。导入以下类:**

`org.apache.spark.SparkContext
org.apache.spark.SparkConf`

**2。创建 SparkConf 对象:**

`val conf = new SparkConf().setMaster("local").setAppName("testApp")`

Master 和 AppName 是运行 spark 应用程序必须设置的最少属性。

**3。使用上一步中创建的 SparkConf 对象创建 SparkContext 对象:**

`val sc = new SparkContext(conf)`

下一步是创建一个集合对象。

让我们看看一些常用的集合，它们可以并行化以形成 RDD:

**数组:**是 Scala 中一种特殊类型的集合。它的大小是固定的，可以存储相同类型的元素。存储在数组中的值是可变的。

可以通过以下方式创建数组:

`var arr=new Array[dataType](size)`

创建变量 arr 后，我们必须在每个索引处插入值。

`var arr=Array(1,2,3,4,5,6)`

这里我们直接提供数组的元素，数据类型和大小是自动推断出来的。

**序列:**序列是 iterable 类的 iterable 集合的特例。但是与 iterables 相反，序列总是有定义好的元素顺序。序列可以通过以下方式创建:

`var mySeq=Seq(1,2,3,4,5,6)`

**列表:**列表类似于数组，因为它们只能有相同类型的元素。但是有两个显著的区别:1)列表的元素不能像数组一样被修改，2)列表代表一个链表。

可以通过以下方式创建列表:

`Val myList=List(1,2,3,4,5,6)`

现在我们有了所有需要的对象，我们可以调用 sparkContext 对象上可用的 parallelize()方法，并将集合作为参数传递。

### Spark 并行化的例子

下面举几个例子:

#### 示例#1

**代码:**

`val conf= new SparkConf().setMaster("local").setAppName("test")
val sc = new SparkContext(conf)
sc.setLogLevel("WARN")
val rdd1= sc.parallelize(Array(1,2,3,4,5))
println("elements of rdd1")
rdd1.foreach(x=>print(x+","))
println()
val rdd2 = sc.parallelize(List(6,7,8,9,10))
println("elements of rdd2")
rdd2.foreach(x=>print(x+","))
println()
val rdd3=rdd1.union(rdd2)
println("elements of rdd3")
rdd3.foreach(x=>print(x+","))`

**输出:**

![Spark Parallelize output 1](../Images/a283230d234678a839d834e412521f99.png)

<noscript><img class="alignnone size-full wp-image-362952" src="../Images/a283230d234678a839d834e412521f99.png" alt="Spark Parallelize output 1" width="272" height="154" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-output-1-1.png"/></noscript>

在上面的代码中，我们首先创建了 sparkContext 对象(sc ),并通过将数组传递给并行化方法创建了 rdd1。然后，我们通过传递一个列表来创建 rdd2，最后，我们通过调用 union 方法 one rdd1 并传递 rdd2 作为参数来合并这两个 rdd。

#### 实施例 2

**代码:**

`val conf= new SparkConf().setAppName("test").setMaster("local")
val sc =new SparkContext(conf)
val spark=SparkSession.builder().config(conf).getOrCreate()
sc.setLogLevel("ERROR")
val line1 = "live life enjoy detox"
val line2="learn apply live motivate"
val line3="life detox motivate live learn"
val rdd =sc.parallelize(Array(line1,line2,line3))
val rdd1 = rdd.flatMap(=>x.split(" "))
import  spark.implicits x._
val df = rdd1.toDF("word")
df.createOrReplaceTempView("tempTable")
val rslt=spark.sql("select word,COUNT(1) from tempTable GROUP BY word ")
rslt.show(1000,false)`

**输出:**

![Spark Parallelize output 2](../Images/e0a716651b8ce77185c7e9e90016ffd1.png)

<noscript><img class="alignnone size-full wp-image-362954" src="../Images/e0a716651b8ce77185c7e9e90016ffd1.png" alt="Spark Parallelize output 2" width="199" height="263" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-output-2-1.png"/></noscript>

上面的代码代表了经典的字数统计程序。我们使用 spark-sql 来实现。为了使用 sql，我们通过调用 toDF 方法将 rdd1 转换为 dataFrame。要使用这种方法，我们必须导入 spark.implicits。我们将 dataFrame(df)注册为临时表，并在其上运行查询。

#### 实施例 3

**代码:**

`val conf= new SparkConf().setAppName("test").setMaster("local")
val sc =new SparkContext(conf)
sc.setLogLevel("ERROR")
val myRdd=sc.parallelize(Seq(1,1,1,10,10,5,100,100,100,200,400),10)
println("Printing myRdd: ")
myRdd.foreach(x=>print(x+" "))
println()
val newRdd= myRdd.distinct()
println("Printing newRdd: ")
newRdd.foreach(x=>print(x+" "))`

**输出:**

![output 3](../Images/b83a24942301d40b3a930c6692b73f7b.png)

<noscript><img class="alignnone wp-image-362955 size-full" src="../Images/b83a24942301d40b3a930c6692b73f7b.png" alt="output 3" width="358" height="110" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-output-3.png 358w, https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-output-3-300x92.png 300w" sizes="(max-width: 358px) 100vw, 358px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/05/Spark-Parallelize-output-3.png"/></noscript>

在上面的代码中，我们使用一个序列创建了一个 RDD(myRdd ),并将 numSlices 作为 10 传递。然后，我们调用了 distinct 方法，它给出了 RDD 中的 distinct 元素。正如预期的那样，输出打印出不同的元素。

### 结论

在本文中，我们学习了如何使用 parallelize()方法创建 rdd。这种方法主要由第一次学习 spark 的初学者使用，在生产环境中，它通常用于编写测试用例。

### 推荐文章

这是 Spark 并行化的指南。这里我们讨论如何使用 Spark 并行化方法和例子，以便更好地理解。您也可以阅读以下文章，了解更多信息——

1.  [火花功能](https://www.educba.com/spark-functions/)
2.  [火花版本](https://www.educba.com/spark-versions/)
3.  [火花元件](https://www.educba.com/spark-components/)
4.  [火花工具](https://www.educba.com/spark-tools/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>