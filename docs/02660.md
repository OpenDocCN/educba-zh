# 决策树超参数

> 原文:[https://www.educba.com/decision-tree-hyperparameters/](https://www.educba.com/decision-tree-hyperparameters/)

![Decision Tree Hyperparameters](../Images/f02e80aa47d94941d64ddb0ada403689.png)

<noscript><img class="alignnone size-full wp-image-499728" src="../Images/f02e80aa47d94941d64ddb0ada403689.png" alt="Decision Tree Hyperparameters" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2021/08/Decision-Tree-Hyperparameters.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2021/08/Decision-Tree-Hyperparameters-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2021/08/Decision-Tree-Hyperparameters-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2021/08/Decision-Tree-Hyperparameters.jpg"/></noscript>

## 决策树超参数简介

决策树超参数被定义为决策树是用于两个任务的机器学习算法:分类和回归。此外，决策树用于在集成学习算法中构建树，超参数是一个参数，其中其值用于控制学习过程。决策树有大量的超参数，需要微调才能得到可能的最佳模型；通过使用它，泛化误差已经被减少，并且搜索超参数空间用于使用它的一组值调整方法将优化我们的模型的结构。

### 各种决策树超参数

下面给出了各种决策树超参数:

<small>Hadoop、数据科学、统计学&其他</small>

#### 1.最大深度

超参数 max_depth 的名称暗示了我们允许树生长到的最大深度。你允许的越深，我们的模型就会变得越复杂；在训练错误的情况下，很容易看到会发生什么。如果我们增加 max_depth 值，训练误差将总是下降，但是让我们考虑测试误差的情况；如果我们将 max_depth 设置得太高，那么在没有捕获有用模式的情况下，决策树可能会简单地过度拟合训练数据，这将导致测试误差增加，但是如果我们将它的值设置得太低，这也不好，那么决策树可能没有什么灵活性来捕获训练数据中的模式和交互。

这也将导致测试误差增加，这是一种欠拟合的情况，因此我们必须使用超参数来找到正确的最大深度，调整网格搜索或随机搜索可能达到的最大深度的最佳可能值。

#### 2.最小 _ 样本 _ 分割

在决策树中，我们有多个节点，其中一些是内部节点，其他的称为叶节点，内部节点将进一步分裂，所以现在让我们看看名为 min_sample_split 的超参数，它指定分裂内部代码所需的最小样本数。 我们可以为这个超参数输入它们的各种值，我们可以指定一个整数或一个数字来表示我们希望内部节点的最小样本数，或者使用内部节点中的分数来表示样本的百分比。

#### 3.最小样本叶

让我们看看另一个参数，称为 min_samples_leaf，因为我们已经看到叶节点是没有任何子节点的节点，所以我们不能进一步分割叶节点，所以 min_samples_leaf 是我们可以指定的最小样本数，以将给定节点称为叶节点，这样我们就不想进一步分割它。

例如，我们从 10，000 个样本开始，到达一个只有 100 个样本的节点；进一步分割是没有意义的，因为你会倾向于过度拟合你所拥有的训练数据，所以通过巧妙地使用这个超参数，我们也可以避免过度拟合，这个参数类似于 min_samples_splits。然而，这描述了作为树的基础的叶子处的最小样本数。

#### 4.最大 _ 功能

这种类型的超参数表示特征的数量；当我们寻找我们可以考虑的最佳分割时，我们可以在每次分割时指定一个数字来表示 max_features，或者有一个分数来表示进行分割时要考虑的功能的百分比，通过创建分割要考虑的功能的百分比。还有各种选项可用。我们可以使用 sqrt、log2，我们也可以指定 none，我们不希望这些 max_features 中的任何一个派上用场。

#### 5.最小重量分数叶

这也是另一种类型的决策树超参数，称为 min_weight_fraction，它是在 sample_weight 确定权重的叶节点处需要的输入样本的分数，这样，我们可以处理类不平衡，并且类不平衡可以通过从每个类中采样相等数量的样本来完成。

此外，当我们将它偏向于不知道样本权重的主导类时，如 min_sample_leaf，通过使用基于权重的标准，如果样本是加权的，min_weight_fraction_leaf，则更容易优化树的结构，其中叶节点包含权重总和的至少一部分。

#### 6.随机状态

这个超参数不是真的要调；因此让我们看看什么时候以及为什么我们需要设置一个 random_state 超参数；许多新生对 random_state 值及其准确性感到困惑；可能发生这种情况是因为决策树的算法基于贪婪算法，该算法通过使用随机选择特征重复多次，并且该选择受伪随机数影响，该伪随机数将 random_state 值作为种子值，使得 random_state 可以随机挑选好的特征，用 random_state 改变模型的准确性意味着模型中有问题；因此，我们可以说它不是一个超参数。

#### 7.类别 _ 重量

这也是一个名为 class_weight 的超参数，其中权重与类相关联，或者用于为每个输出类提供权重；这实际上意味着，当算法计算杂质以在每个节点进行分割时，通过给定子样本权重，产生的子节点由 class_weight 进行加权，我们的类的分布已经开始，然后是类的权重，然后根据我们的树倾斜的位置，我们可以尝试增加另一个类的权重，以便算法相对于另一个类惩罚一个类的样本。

### 结论

在本文中，我们得出结论，决策树的前五个超参数非常重要，它们在构建最准确的模型中起着至关重要的作用。我们已经看到了超参数的工作、超参数的调整以及它们如何相互作用，没有必要调整每个超参数。

### 推荐文章

这是决策树超参数的指南。这里我们分别讨论介绍和各种决策树超参数。您也可以看看以下文章，了解更多信息–

1.  [什么是决策树？](https://www.educba.com/what-is-decision-tree/)
2.  数据挖掘中的决策树
3.  [创建决策树](https://www.educba.com/create-decision-tree/)
4.  [决策树算法](https://www.educba.com/decision-tree-algorithm/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>