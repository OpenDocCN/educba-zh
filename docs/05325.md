# PySpark 计数

> 原文：<https://www.educba.com/pyspark-count/>

![PySpark Count](img/c92c8e2a17def5f9ce3bb57bdb7eb9dd.png)



## PySpark 计数简介

PySpark Count 是一个 PySpark 函数，用于计算 PySpark 数据模型中的元素数量。此 count 函数用于返回数据中元素的数量。这是 PySpark 中的一个动作操作，它计算 PySpark 数据模型中的行数。这是一个重要的操作数据模型，用于进一步的数据分析，计算要使用的元素数量。count 函数对数据进行计数，并将数据返回给 PySpark 中的驱动程序，使 type 在 PySpark 中动作。PySpark 中的 count 函数用于计算数据框后期/前期数据分析中出现的行数。

**语法:**

<small>网页开发、编程语言、软件测试&其他</small>

```
b.count()
```

*   **b:** 创建的数据帧。
*   **count():** 对数据框模型中存在的数据元素进行计数的计数操作。

**输出:**

![b.count()](img/7c0b95c2f352a3daa1ea55b24d3c8740.png)



### PySpark 中的计数工作

*   count 是 PySpark 中的一个操作，用于计算 PySpark 数据模型中的元素数量。它是 PySpark 中的一个分布式模型，其中动作是分布式的，所有的数据都被带回驱动节点。数据混洗操作有时会使数据模型的计数操作代价更高。
*   当应用于数据集时，计数操作通过一个执行器聚合数据，而 RDD 上的计数操作在驱动程序中聚合数据最终结果。这构成了数据集的两个阶段和 RDD 的一个阶段。通过显式缓存数据，数据将是可用的，并且数据不会在内存中。

### PySpark 计数示例

下面提到了不同的例子:

但是，首先，让我们在 PySpark 中创建一个样本数据框。

**代码:**

```
data1 = [{'Name':'Jhon','Sal':25000,'Add':'USA'},{'Name':'Joe','Sal':30000,'Add':'USA'},{'Name':'Tina','Sal':22000,'Add':'IND'},{'Name':'Jhon','Sal':15000,'Add':'USA'}]
```

该数据包含姓名、工资和地址，将用作创建数据框的样本数据。

```
a = sc.parallelize(data1)
```

sc.parallelize 将用于使用给定数据创建 RDD。

```
b = spark.createDataFrame(a)
```

创建后，我们将使用 createDataFrame 方法来创建数据框。

这是数据框的外观。

```
b.show()
```

**输出:**

![PySpark Count 2](img/41b6bdfb23cf2efffcef11af1ace9bf7.png)



现在，让我们尝试使用 Dataframe.count()函数来计算数据帧中的元素数量。这些计数会创建一个 DAG，并将数据带回驱动程序节点以供运行。

```
b.count()
```

这将对数据帧中出现的数据元素进行计数，并将结果返回给驱动程序。

**输出:**

![number of elements](img/63910ff035269937d06c305a9f216961.png)



现在，让我们通过创建一个包含元素的火花 RDD 来尝试计数元素。这将创建一个 RDD，并对该特定 RDD 数据模型中存在的数据元素进行计数。

我们采用的 RDD 可以是任何现有的数据类型，count 函数可以处理它。

```
a = sc.parallelize(["Ar","Br","Cr","Dr"])
a.count()
```

现在，让我们通过将数据类型作为整数来尝试做到这一点。这将再次制作一个 RDD，并计算其中存在的元素。请注意，所有元素都使用 count 函数进行计数，不仅是不同的元素，即使有重复值，这些元素也将作为 PySpark 数据模型中 Count 函数的一部分进行计数。

```
a = sc.parallelize([2,3,4,56,3,2,4,5,3,4,56,4,2])
a.count()
```

**输出:**

![data type as Integer](img/f384560c93b643443c6d607e7e9eb09e.png)



**Note:** It is an action operation in PySpark. It returns the count of elements present in the PySpark data model. Second, an action operation brings back the data to the driver node, so shuffling of data happens. Finally, it initiates DAG execution in PySpark Data Frame.

### 结论

从上面的文章中，我们看到了 PySpark 中 Count 的工作原理。从各种示例和分类中，我们试图了解这些计数在 PySpark 中是如何使用的，以及在编程级别上是如何使用的。所使用的各种方法显示了它如何简化数据分析的模式以及同样的成本效益模型。我们还看到了计数数据帧的内部工作和优点，以及它在各种编程目的中的使用。此外，语法和例子帮助我们更精确地理解了函数。

### 推荐文章

这是 PySpark 计数指南。为了更好地理解，我们在这里讨论 PySpark 中计数的介绍、工作原理和示例。您也可以看看以下文章，了解更多信息–

1.  [PySpark 回合](https://www.educba.com/pyspark-round/)
2.  [PySpark 列到列表](https://www.educba.com/pyspark-column-to-list/)
3.  [PySpark 选择列](https://www.educba.com/pyspark-select-columns/)
4.  [PySpark 加入](https://www.educba.com/pyspark-join/)





