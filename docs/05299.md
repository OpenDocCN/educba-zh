# 火花拼花地板

> 原文:[https://www.educba.com/spark-parquet/](https://www.educba.com/spark-parquet/)

![Spark Parquet](../Images/8430a35280cc6734252ef0be427c6faf.png)

<noscript><img class="alignnone size-full wp-image-376759" src="../Images/8430a35280cc6734252ef0be427c6faf.png" alt="Spark Parquet" width="900" height="500" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet.jpg 900w, https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-300x167.jpg 300w, https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-768x427.jpg 768w" sizes="(max-width: 900px) 100vw, 900px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet.jpg"/></noscript>

## 火花拼花地板简介

许多数据处理系统都支持 Parquet。这是一种战斗形式。火花驱动分析的深层组成部分被定义为拼花地板。与拼花地板相关的优势很强。其中包括存储效率、快速、高效的数据查询，以及附带的一些工具。通信成本(输入和输出限制)和数据解码(CPU 限制)是分布分析的主要瓶颈，使用 Spark SQL Parquet 已经克服了这些瓶颈。Spark SQL 支持提供对 Parquet 文件的读写。并且它们自动捕获原始数据方案。

**语法:**

<small>Hadoop、数据科学、统计学&其他</small>

`spark.write.parquet()`

这是 Spark 拼花数据框的语法。

### 阿帕奇 Spark 拼花地板是如何工作的？

二进制是拼花地板中使用的格式。拼花格式基本上是经过编码和压缩的。使用的文件是分栏的。Spark 的这个 SQL 是机器友好的。JVM、Hadoop 和 C++是使用的 API。让我们在理解 Apache Spark 拼花地板时考虑一个例子。在开始之前，首先，我们需要从 Seq 对象为 Spark 创建一个数据框。只有在序列上使用 toDF()数据框函数导入隐含时，才可使用–spark . sqlcontext . implicits . _

`val entry = Seq(("John ","Hen","Kennedy","36636","M",8000),
("Michael ","","Rose","40288","F",10000),
("Robert ","Patrick","Williams","42114","M",5000),
(" ","Anne","Frank","39192","F",9000),
("Robert","Downey","Junior","","M",12554678966351))
val cols = Seq("first_name","mid_name","last_name","date of birth","gender","income")
import spark.sqlContext.implicits._
val dataframe_ = entry.toDF(cols:_*)`

**1** 。拼花文件的数据框布线:

`df.write.parquet(“C:temp/file_output/persoal.parquet”)`

**2。**为了兼容，所有的列都被自动转换为可空。Parquet 中的数据输入保留了列的数据类型和名称。此外，通过将所有数据写入指定的文件夹来完成。拼花文件的数据框创建:

`val parquet.data.frame = spark.read.parquet(“C:temp/file_output/personal.parquet”)`

**3。**查询拼花文件的 SQL 使用情况:拼花的临时视图也可以被创建并在 SQL 语句中使用。

`parquet.data.frame.createOrReplaceViewTemp(“Table.Of.Parquet”)
val parquet.SQL = spar.sql(“select from table.of.parquet where income >=4000 ”)`

**4。**与传统的数据库表扫描一样，具有瓶颈性能的文件扫描是由上述对 Parquet 文件的谓词完成的。使用分区来提高性能是必要的。

**5。**通过分区提高性能:Spark 作业在可伸缩区域工作是由分区引起的，因为分区是许多数据库和数据处理框架的一个特性。

`data.fram.write.partitionBy(“gender”, “income”)spark.read.parquet(“C:temp/file_output/personal2.parquet”)`

分区文件夹的层次结构如下图所示:

![Spark Parquet - 1](../Images/9ee9d4b6899d76e423786c1a63531111.png)

<noscript><img class="alignnone size-full wp-image-376681" src="../Images/9ee9d4b6899d76e423786c1a63531111.png" alt="Spark Parquet - 1" width="601" height="207" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-1.png 601w, https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-1-300x103.png 300w" sizes="(max-width: 601px) 100vw, 601px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-1.png"/></noscript>

以上内容可以看作是一个示例，用于了解如何将 Spark SQL 的数据帧写入 Parquet 文件，该文件保留了性别和收入列中的分区。

`val parquet.data.frame = spark.read.parquet(“C:temp/file_output/personal.parquet”)
parquet.data.frame.createOrReplaceViewTemp(“Table2.Of.Parquet”)val parquet.SQL = spar.sql(“select from table.of.parquet where gender = ‘MALE’ and income >=4000 ”)`

在没有分区的情况下执行查询比没有分区的查询更容易、更快。从 SQL parquet 读取特定分区:

`val parquet.data.frame = spark.read.parquet(“C:temp/file_output/personal2.parquet / gender = ‘Male’”)`

这有助于从按性别划分的男性中检索数据。

**输出:**

![Spark Parquet - 2](../Images/a3f3e75d231d73ad34efba8511cd99b6.png)

<noscript><img class="alignnone size-full wp-image-376691" src="../Images/a3f3e75d231d73ad34efba8511cd99b6.png" alt="Spark Parquet - 2" width="426" height="377" srcset="https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-2.png 426w, https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-2-300x265.png 300w" sizes="(max-width: 426px) 100vw, 426px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2020/06/Spark-Parquet-2.png"/></noscript>

### 优势

Spark 中 Apache 拼花地板的优点和用途:

*   按列输入输出操作存储限制。
*   通过列存储获取需要访问的特定列。
*   列存储占用的存储空间更少。
*   列存储和特定于类型的编码可以更好地汇总数据。

### 结论

我们在 Spark 架构中看到了洗牌的概念。洗牌操作非常迅速，根本不需要排序。有时不需要维护哈希表。当包含在地图中时，会在地图端创建少量数据或文件。随机输入输出操作，少量是必需的，大部分是顺序读写。

### 推荐文章

这是一份星火拼花地板指南。这里我们讨论一下 Spark Parquet 的介绍，语法，它是如何工作的，用例子来实现。您也可以浏览我们的其他相关文章，了解更多信息——

1.  [火花工具](https://www.educba.com/spark-tools/?source=leftnav)
2.  [火花外壳命令](https://www.educba.com/spark-shell-commands/?source=leftnav)
3.  [火花功能](https://www.educba.com/spark-functions/?source=leftnav)
4.  [星火中的 RDD](https://www.educba.com/rdd-in-spark/?source=leftnav)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>