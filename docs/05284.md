# Apache Spark 架构

> 原文:[https://www.educba.com/apache-spark-architecture/](https://www.educba.com/apache-spark-architecture/)

![Apache Spark Architecture](../Images/125ce697fe77f6dbb2016270b42fdcfa.png)

<noscript><img class="alignnone size-full wp-image-242175" src="../Images/125ce697fe77f6dbb2016270b42fdcfa.png" alt="Apache Spark Architecture" width="900" height="500" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/11/Apache-Spark-Architecture.png"/></noscript>

## Apache Spark 架构介绍

Apache Spark Architecture 是一个基于开源框架的组件，用于处理大量非结构化、半结构化和结构化数据进行分析。Spark 架构被认为是 Hadoop 和 map-reduce 架构的替代方案，用于大数据处理。Spark 架构与弹性分布式数据集(RDD)和有向无环图(DAG)相关联，用于数据存储和处理。此外，它有四个组成部分，是该架构的一部分，如火花驱动程序、执行器、集群管理器和工作节点。Spark 使用数据集和数据帧作为主要的数据存储组件，有助于优化 Spark 流程和大数据计算。

### Apache Spark 架构

Apache spark 的架构具有松散耦合的组件。Spark 考虑架构中的主/工作进程，所有任务都在 Hadoop 分布式文件系统的顶层工作。Apache spark 利用 Hadoop 进行数据处理和数据存储过程。他们被认为是内存中的数据处理引擎，使他们的应用程序在 Hadoop 集群上的运行速度比内存更快。内存处理可以防止磁盘 I/O 故障。Spark 允许异构作业处理相同的数据。Spark 将其数据划分为分区，分区的大小取决于给定的数据源。

<small>Hadoop、数据科学、统计学&其他</small>

下面是 Apache Spark 架构的两个主要实现:

#### 1.弹性分布式数据集(RDD)

它负责提供控制缓存和分区的 API。它是数据计算的一个重要工具集。它有助于在失败的情况下重新计算元素，并且被认为是不可变的数据，充当接口。变换和行动是 RDD 完成的两个操作。

#### 2.有向无环图

它形成了从一个节点到另一个节点的顺序连接。驱动程序将程序转换为每个作业的 DAG。Apache Spark 生态系统具有各种组件，如 API 核心、Spark SQL、流和实时处理、MLIB 和 Graph X。这里要学习的一些术语是 Spark shell，它有助于读取大量数据、Spark 上下文取消、运行作业、任务(工作)、作业(计算)

### Apache Spark 架构的组件

下面给出了 Spark 的四个主要组件，有必要了解它们以获得完整的框架。

1.  火花驱动器
2.  实施者
3.  集群管理器
4.  工作节点

下图显示了 spark 的架构和组件:

![Apache Spark Architecture](../Images/5a2b4084e33eb955a036d73bae7e6399.png)

<noscript><img class="alignnone size-full wp-image-242627" src="../Images/5a2b4084e33eb955a036d73bae7e6399.png" alt="Apache Spark Architecture" width="632" height="596" srcset="https://cdn.educba.com/academy/wp-content/uploads/2019/11/Apache-Spark-Architecture-1.png 632w, https://cdn.educba.com/academy/wp-content/uploads/2019/11/Apache-Spark-Architecture-1-300x283.png 300w" sizes="(max-width: 632px) 100vw, 632px" data-original-src="https://cdn.educba.com/academy/wp-content/uploads/2019/11/Apache-Spark-Architecture-1.png"/></noscript>

图:Apache Spark 架构的独立模式

执行流程开始如下:

#### 1.火花驱动器

司机的责任是协调任务和工人进行管理。这是一个应用程序 JVM 进程，被认为是一个主节点。驱动程序将 spark 分成任务和时间表，在集群中的执行器上执行。在该图中，驱动程序调用主应用程序并创建 spark 上下文(充当网关),共同监控给定集群内的工作，并连接到 spark 集群。所有功能和命令都通过 Spark 上下文完成。

Spark 上下文是每个会话的一个条目。Spark driver 有更多的组件来执行集群中的作业。Spark 集群连接到不同类型的集群管理器，同时上下文获取工作节点来执行和存储数据。在集群中，当我们执行流程时，他们的工作被细分为多个阶段，每个阶段又被划分为多个预定任务。

#### 2.执行者

它负责执行作业，并将数据存储在缓存中。在最初阶段，执行者向司机登记。这个执行器有许多时隙来并发运行应用程序。执行器对外部源执行读/写过程。执行器在加载了数据并在空闲模式下删除数据后运行作业。执行器通过动态分配来启用，并且根据持续时间不断地被包括和排除。在任务执行期间，执行器由驱动程序监控。执行器在 java 进程中执行用户的任务。

#### 3.集群管理器

它有助于管理只有一个主机和多个从机的集群。有两种类型的集群管理器，如 YARN 和 standalone，它们都由资源管理器和节点管理。独立集群工作需要 Spark Master 和 worker node 作为它们的角色。集群管理器的职责是分配资源和执行任务，

#### 4.工作节点

它们是从节点；主要职责是执行任务，任务的输出返回给 spark 上下文。它们与主节点就资源的可用性进行通信。Spark context 执行它并发布给 worker 节点。每个 worker 节点被分配一个 spark worker 进行监控。他们通过增加工作节点(1 到 n 个工作节点)使计算变得非常简单，这样，通过将作业划分到多个系统上的分区中，所有任务都可以并行执行。其他元素任务被认为是一个工作单元，分配给一个执行器，每个分区 spark 运行一个任务。

### 结论

因此，通过理解 Apache Spark 架构，它表明了如何以一种简单的方式实现大数据。最终，我们了解了它们的可访问性及其组件角色，这对集群计算和大数据技术非常有益。Spark 以一种更简单的方式计算所需的结果，是批处理中的首选。

Spark 的独特功能，如数据集和数据框，有助于优化用户的代码。像 SQL engine 这样的重要特性提升了执行速度，让这个软件变得多才多艺。因此，我们已经看到 spark 应用程序在本地运行或分布在一个集群中。Apache Spark 被认为是大数据等广泛行业的重要补充。总之，spark 有助于解决高计算任务。

### 推荐文章

这是一个 Apache Spark 架构的指南。这里我们讨论 Apache Spark 架构的介绍，以及 Apache Spark 的组件和框图。您也可以浏览我们推荐的其他文章，了解更多信息——

1.  [火花外壳命令](https://www.educba.com/spark-shell-commands/)
2.  [Apache Hadoop 生态系统](https://www.educba.com/apache-hadoop-ecosystem/)
3.  [大数据架构](https://www.educba.com/big-data-architecture/)
4.  [什么是阿帕奇](https://www.educba.com/what-is-apache/)

<footer class="entry-footer">

<aside class="sidebar sidebar-primary widget-area" role="complementary" aria-label="Primary Sidebar">Primary Sidebar</aside>

</footer>